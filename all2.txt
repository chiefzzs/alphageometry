/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
2024-01-22 03:14:51.868527: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
I0122 03:14:53.441141 139878876468288 inference_utils.py:69] Parsing gin configuration.
I0122 03:14:53.441234 139878876468288 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0122 03:14:53.441420 139878876468288 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0122 03:14:53.441457 139878876468288 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0122 03:14:53.441497 139878876468288 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0122 03:14:53.441526 139878876468288 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0122 03:14:53.441554 139878876468288 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0122 03:14:53.441587 139878876468288 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0122 03:14:53.441615 139878876468288 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0122 03:14:53.441642 139878876468288 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=2
I0122 03:14:53.441680 139878876468288 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0122 03:14:53.441724 139878876468288 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0122 03:14:53.441793 139878876468288 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0122 03:14:53.441986 139878876468288 resource_reader.py:55] Path not found: base_htrans.gin
I0122 03:14:53.442196 139878876468288 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0122 03:14:53.442368 139878876468288 resource_reader.py:55] Path not found: trainer_configuration.gin
I0122 03:14:53.447754 139878876468288 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0122 03:14:53.447896 139878876468288 resource_reader.py:55] Path not found: size/medium_150M.gin
I0122 03:14:53.448239 139878876468288 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0122 03:14:53.448366 139878876468288 resource_reader.py:55] Path not found: options/positions_t5.gin
I0122 03:14:53.448660 139878876468288 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0122 03:14:53.448795 139878876468288 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0122 03:14:53.449281 139878876468288 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0122 03:14:53.449410 139878876468288 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0122 03:14:53.453449 139878876468288 training_loop.py:334] ==== Training loop: initializing model ====
I0122 03:14:53.455962 139878876468288 xla_bridge.py:166] Remote TPU is not linked into jax; skipping remote TPU.
I0122 03:14:53.456056 139878876468288 xla_bridge.py:413] Unable to initialize backend 'tpu_driver': Could not initialize backend 'tpu_driver'
I0122 03:14:53.456129 139878876468288 xla_bridge.py:413] Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
I0122 03:14:53.456189 139878876468288 xla_bridge.py:413] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
I0122 03:14:53.456602 139878876468288 xla_bridge.py:413] Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.
I0122 03:14:53.456677 139878876468288 xla_bridge.py:413] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
W0122 03:14:53.456734 139878876468288 xla_bridge.py:420] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0122 03:14:53.456807 139878876468288 training_loop.py:335] Process 0 of 1
I0122 03:14:53.456863 139878876468288 training_loop.py:336] Local device count = 1
I0122 03:14:53.456926 139878876468288 training_loop.py:337] Number of replicas = 1
I0122 03:14:53.456976 139878876468288 training_loop.py:339] Using random number seed 42
I0122 03:14:53.657796 139878876468288 training_loop.py:359] Initializing the model.
I0122 03:14:53.767574 139878876468288 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.767827 139878876468288 decoder_stack.py:316] dstack: scanning over 1 windows.
I0122 03:14:53.767914 139878876468288 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:14:53.767970 139878876468288 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:14:53.768023 139878876468288 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:14:53.768067 139878876468288 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:14:53.768117 139878876468288 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:14:53.768167 139878876468288 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:14:53.768217 139878876468288 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:14:53.768287 139878876468288 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:14:53.768358 139878876468288 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:14:53.768428 139878876468288 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:14:53.768498 139878876468288 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:14:53.768584 139878876468288 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:14:53.768656 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:53.768726 139878876468288 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0122 03:14:53.768891 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:53.768946 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:53.769000 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:53.771330 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.779790 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:53.799537 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.800030 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:53.806443 139878876468288 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:14:53.823745 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:53.823869 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:53.823931 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:53.823992 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.824109 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.825628 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.825783 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.827029 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.829943 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.836580 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.838010 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.838161 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:53.838216 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:53.838289 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.838553 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:53.838914 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:53.838977 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:53.843813 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.843974 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:53.846985 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.847182 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:53.847628 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:53.862993 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:53.876960 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.877112 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:53.877596 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.877754 139878876468288 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0122 03:14:53.877898 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:53.877949 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:53.878000 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:53.880631 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.884399 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:53.901820 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.902751 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:53.906266 139878876468288 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:14:53.914163 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:53.914295 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:53.914366 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:53.914447 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.914579 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.915056 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.915161 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.915698 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.917024 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.920685 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.921266 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.921393 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:53.921458 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:53.921544 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.921863 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:53.922269 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:53.922342 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:53.927324 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.927510 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:53.931082 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.931251 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:53.931693 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:53.936810 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:53.941960 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.942133 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:53.942542 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.942657 139878876468288 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0122 03:14:53.942789 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:53.942851 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:53.942905 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:53.945153 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.948625 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:53.963265 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.964019 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:53.967974 139878876468288 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:14:53.973472 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:53.973572 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:53.973640 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:53.973707 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.973803 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.974302 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.974394 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.974769 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.975577 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.977786 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.978127 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.978209 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:53.978268 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:53.978351 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.978600 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:53.978947 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:53.979007 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:53.983106 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.983198 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:53.985498 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.985582 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:53.985901 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:53.989609 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:53.992941 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.993041 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:53.993288 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:53.993371 139878876468288 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0122 03:14:53.993464 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:53.993506 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:53.993539 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:54.081120 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.083516 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:54.092614 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.093055 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:54.095360 139878876468288 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:14:54.099626 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:54.099682 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:54.099723 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:54.099773 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.099856 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.100170 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.100249 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.100572 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.101716 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.104145 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.104472 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.104552 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:54.104591 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:54.104651 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.104858 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:54.105197 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:54.105259 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.108525 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.108614 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.110887 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.111000 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:54.111292 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:54.114531 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.117371 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.117461 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.117711 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.117800 139878876468288 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0122 03:14:54.117899 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:54.117949 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:54.117985 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:54.120033 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.122362 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:54.132545 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.133015 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:54.135335 139878876468288 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:14:54.139871 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:54.139972 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:54.140027 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:54.140081 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.140168 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.140661 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.140755 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.141219 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.142352 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.145509 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.145847 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.145928 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:54.145967 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:54.146029 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.146279 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:54.146633 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:54.146692 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.149660 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.149749 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.152096 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.152188 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:54.152485 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:54.155815 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.158780 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.158879 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.159134 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.159224 139878876468288 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0122 03:14:54.159324 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:54.159372 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:54.159408 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:54.161346 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.164442 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:54.173039 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.173475 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:54.175845 139878876468288 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:14:54.180080 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:54.180165 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:54.180211 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:54.180264 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.180371 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.180789 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.180895 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.181350 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.182158 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.184345 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.184699 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.184793 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:54.184833 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:54.184892 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.185126 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:54.185461 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:54.185521 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.188982 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.189087 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.191425 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.191510 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:54.191806 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:54.195080 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.197990 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.198080 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.198416 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.198526 139878876468288 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0122 03:14:54.198636 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:54.198688 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:54.198737 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:54.200671 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.203832 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:54.212339 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.212768 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:54.215094 139878876468288 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:14:54.219295 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:54.219372 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:54.219429 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:54.219478 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.219584 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.220010 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.220114 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.220564 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.221725 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.223928 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.224298 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.224383 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:54.224423 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:54.224487 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.224743 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:54.225101 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:54.225168 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.228370 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.228506 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.230987 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.231108 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:54.231421 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:54.234808 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.237834 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.237957 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.238220 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.238313 139878876468288 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0122 03:14:54.238427 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:54.238485 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:54.238544 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:54.239934 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.243445 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:54.252934 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.253399 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:54.255726 139878876468288 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:14:54.260280 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:54.260365 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:54.260408 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:54.260447 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.260536 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.260865 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.260944 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.261266 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.262326 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.264533 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.264849 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.264930 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:54.264969 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:54.265030 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.265253 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:54.265590 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:54.265651 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.268809 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.268899 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.271357 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.271444 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:54.271740 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:54.275006 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.277853 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.277946 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.278205 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.278299 139878876468288 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0122 03:14:54.278430 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:54.278486 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:54.278543 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:54.280547 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.283627 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:54.293000 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.293463 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:54.295728 139878876468288 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:14:54.300189 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:54.300263 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:54.300304 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:54.300342 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.300421 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.300719 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.300791 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.301093 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.301846 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.303973 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.304301 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.304387 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:54.304423 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:54.304470 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.304641 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:54.304869 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:54.304915 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.307796 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.307891 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.310182 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.310258 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:54.310544 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:54.313798 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.316634 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.316734 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.317025 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.317126 139878876468288 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0122 03:14:54.317218 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:54.317258 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:54.317292 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:54.318632 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.320743 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:54.329269 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.329683 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:54.332026 139878876468288 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:14:54.336560 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:54.336658 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:54.336703 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:54.336752 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.336836 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.337198 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.337279 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.337643 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.338467 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.340834 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.341249 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.341362 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:54.341427 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:54.341512 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.341807 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:54.342194 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:54.342259 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.345433 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.345576 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.348254 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.348379 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:54.348700 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:54.352190 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.355682 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.355804 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.356062 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.356160 139878876468288 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0122 03:14:54.356273 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:54.356324 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:54.356376 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:54.357771 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.360200 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:54.368728 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.369147 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:54.371439 139878876468288 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:14:54.375875 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:54.375960 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:54.376024 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:54.376086 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.376173 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.376511 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.376599 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.376937 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.377712 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.379929 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.380254 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.380335 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:54.380390 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:54.380465 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.380660 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:54.380918 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:54.380972 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.383907 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.383998 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.386278 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.386363 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:54.386680 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:54.389937 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.392854 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.392950 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.393222 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.393316 139878876468288 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0122 03:14:54.393427 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:54.393478 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:54.393531 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:54.394915 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.397102 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:54.405531 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.405967 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:54.408604 139878876468288 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:14:54.412813 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:54.412898 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:54.412962 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:54.413025 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.413115 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.413503 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.413585 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.413939 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.414757 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.417202 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.417578 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.417668 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:54.417723 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:54.417800 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.418010 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:54.418273 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:54.418328 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.421796 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.421944 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.424408 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.424518 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:54.424830 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:54.428172 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.431076 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.431174 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.431438 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.431692 139878876468288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:14:54.431754 139878876468288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:14:54.431814 139878876468288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:14:54.431881 139878876468288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:14:54.431944 139878876468288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:14:54.432007 139878876468288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:14:54.432066 139878876468288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:14:54.432127 139878876468288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:14:54.432186 139878876468288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:14:54.432247 139878876468288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:14:54.432305 139878876468288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:14:54.432367 139878876468288 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:14:54.432419 139878876468288 decoder_stack.py:344] dstack: Final layernorm.
I0122 03:14:54.436991 139878876468288 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:14:54.501322 139878876468288 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.501448 139878876468288 decoder_stack.py:333] dstack: autoregressive generator.
I0122 03:14:54.501511 139878876468288 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0122 03:14:54.501616 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:54.501666 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:54.501713 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:54.501785 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.504701 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:54.514069 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.514657 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:54.516926 139878876468288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:14:54.533167 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:54.533259 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:54.533305 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:54.533356 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.533438 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.534679 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.534775 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.535817 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.537919 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.542688 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.543626 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.543713 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:54.543752 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:54.543821 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.544078 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:54.544210 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:54.544266 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.549032 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.549145 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.551666 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.551752 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:54.551856 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:54.556787 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.559957 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.560062 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.560312 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.560404 139878876468288 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0122 03:14:54.560518 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:54.560567 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:54.560603 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:54.560672 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.563532 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:54.578289 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.578985 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:54.582604 139878876468288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:14:54.596889 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:54.596977 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:54.597021 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:54.597060 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.597152 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.597579 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.597673 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.598124 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.598904 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.601090 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.601460 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.601552 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:54.601600 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:54.601669 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.601862 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:54.601958 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:54.602016 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.605753 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.605873 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.608467 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.608585 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:54.608687 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:54.612158 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.615211 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.615299 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.615549 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.615634 139878876468288 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0122 03:14:54.615731 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:54.615783 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:54.615817 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:54.615865 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.617826 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:54.627834 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.628268 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:54.630475 139878876468288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:14:54.642772 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:54.642865 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:54.642908 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:54.642956 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.643038 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.643357 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.643435 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.643875 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.644805 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.647023 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.647335 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.647415 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:54.647454 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:54.647516 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.647736 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:54.647867 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:54.647923 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.652531 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.652664 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.655762 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.655946 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:54.656118 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:54.659658 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.662718 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.662813 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.663062 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.663147 139878876468288 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0122 03:14:54.663238 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:54.663286 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:54.663330 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:54.663379 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.665355 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:54.675197 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.675642 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:54.677865 139878876468288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:14:54.690344 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:54.690432 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:54.690474 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:54.690513 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.690609 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.690928 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.691002 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.691322 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.691987 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.694164 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.694529 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.694605 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:54.694643 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:54.694691 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.694881 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:54.694977 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:54.695024 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.698175 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.698260 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.700485 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.700570 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:54.700667 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:54.704252 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.707791 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.707902 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.708152 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.708239 139878876468288 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0122 03:14:54.708330 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:54.708378 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:54.708412 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:54.708468 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.710453 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:54.719619 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.720065 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:54.722767 139878876468288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:14:54.734507 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:54.734648 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:54.734715 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:54.734778 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.734873 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.735221 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.735324 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.735774 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.736712 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.739693 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.740094 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.740187 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:54.740225 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:54.740279 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.740472 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:54.740574 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:54.740615 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.743803 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.743888 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.745957 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.746050 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:54.746139 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:54.749749 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.752959 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.753074 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.753323 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.753407 139878876468288 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0122 03:14:54.753500 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:54.753541 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:54.753582 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:54.753649 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.756202 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:54.765288 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.765717 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:54.768002 139878876468288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:14:54.780717 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:54.780823 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:54.780869 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:54.780920 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.781034 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.781444 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.781537 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.782028 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.782885 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.785102 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.785446 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.785542 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:54.785581 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:54.785645 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.785891 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:54.786022 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:54.786079 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.790104 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.790250 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.792465 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.792553 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:54.792650 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:54.796086 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.799141 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.799241 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.799493 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.799585 139878876468288 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0122 03:14:54.799684 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:54.799731 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:54.799767 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:54.799825 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.802616 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:54.812593 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.813072 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:54.815325 139878876468288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:14:54.829660 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:54.829765 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:54.829826 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:54.829892 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.829990 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.830420 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.830513 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.830983 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.831930 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.834956 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.835319 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.835407 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:54.835454 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:54.835536 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.835821 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:54.835971 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:54.836030 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.840033 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.840153 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.842339 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.842456 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:54.842592 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:54.846506 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.849796 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.849933 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.850325 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.850446 139878876468288 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0122 03:14:54.850593 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:54.850651 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:54.850699 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:54.850766 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.853214 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:54.863042 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.863485 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:54.865722 139878876468288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:14:54.878441 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:54.878536 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:54.878582 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:54.878622 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.878703 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.879017 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.879091 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.879410 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.880073 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.882250 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.882573 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.882651 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:54.882688 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:54.882740 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.882944 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:54.883042 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:54.883090 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.886270 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.886400 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.889071 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.889188 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:54.889292 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:54.892805 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.895856 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.895947 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.896198 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.896284 139878876468288 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0122 03:14:54.896382 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:54.896425 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:54.896458 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:54.896505 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.898481 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:54.908380 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.908833 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:54.911055 139878876468288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:14:54.923424 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:54.923511 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:54.923554 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:54.923594 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.923676 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.923993 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.924066 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.924400 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.925068 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.927266 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.927579 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.927653 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:54.927690 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:54.927744 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.927931 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:54.928027 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:54.928074 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.931244 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.931338 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.933454 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.933535 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:54.933633 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:54.937957 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.941153 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.941297 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.941657 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.941775 139878876468288 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0122 03:14:54.941905 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:54.941966 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:54.942016 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:54.942089 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.945203 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:54.955272 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.955756 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:54.958005 139878876468288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:14:54.970317 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:54.970438 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:54.970511 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:54.970584 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.970690 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.971139 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.971238 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.971708 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.972781 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.976028 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.976490 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.976596 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:54.976653 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:54.976737 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.977038 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:54.977198 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:54.977274 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.982622 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.982792 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.986158 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.986325 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:54.986475 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:54.992179 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:54.997340 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.997516 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:54.997920 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:54.998041 139878876468288 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0122 03:14:54.998183 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:54.998247 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:54.998298 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:54.998367 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.001567 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:55.013434 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.013926 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:55.016364 139878876468288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:14:55.029647 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:55.029755 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:55.029827 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:55.029896 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.030022 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.030548 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.030647 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.031312 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.032602 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.035709 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.036234 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.036339 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:55.036395 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:55.036468 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.036746 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:55.036882 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:55.036936 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:55.040874 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.041029 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:55.046077 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.046254 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:55.046409 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:55.051018 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:55.057105 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.057308 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:55.057841 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.057991 139878876468288 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0122 03:14:55.058168 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:55.058241 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:55.058305 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:55.058395 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.063007 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:55.075756 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.076247 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:55.078697 139878876468288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:14:55.094988 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:55.095086 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:55.095157 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:55.095224 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.095323 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.095687 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.095769 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.096133 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.096919 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.099197 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.099573 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.099661 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:55.099719 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:55.099802 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.100027 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:55.100175 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:55.100244 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:55.106489 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.106685 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:55.109585 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.109718 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:55.109830 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:55.114387 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:55.119815 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.119957 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:55.120419 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.120563 139878876468288 decoder_stack.py:344] dstack: Final layernorm.
I0122 03:14:55.123963 139878876468288 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:14:55.247126 139878876468288 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.247266 139878876468288 decoder_stack.py:333] dstack: autoregressive generator.
I0122 03:14:55.247345 139878876468288 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0122 03:14:55.247467 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:55.247520 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:55.247577 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:55.247663 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.251325 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:55.261546 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.262018 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:55.264382 139878876468288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:14:55.280405 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:55.280531 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:55.280599 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:55.280667 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.280776 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.281228 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.281331 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.281805 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.282814 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.285397 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.285812 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.285905 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:55.285954 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:55.286027 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.286306 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:55.286456 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:55.286515 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:55.290024 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.290173 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:55.292663 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.292817 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:55.292953 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:55.296709 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:55.300211 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.300355 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:55.300636 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.300730 139878876468288 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0122 03:14:55.300837 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:55.300885 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:55.300922 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:55.300990 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.303881 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:55.318276 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.318773 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:55.321190 139878876468288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:14:55.333128 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:55.333229 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:55.333292 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:55.333368 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.333489 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.333823 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.333903 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.334353 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.335210 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.337716 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.338186 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.338312 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:55.338376 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:55.338452 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.338717 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:55.338887 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:55.338936 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:55.342184 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.342289 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:55.344462 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.344553 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:55.344653 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:55.348108 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:55.351209 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.351318 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:55.351628 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.351744 139878876468288 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0122 03:14:55.351886 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:55.351952 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:55.352009 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:55.352064 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.354205 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:55.363740 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.364196 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:55.366404 139878876468288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:14:55.377759 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:55.377825 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:55.377866 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:55.377905 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.377983 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.378295 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.378368 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.378719 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.379369 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.381778 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.382084 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.382159 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:55.382196 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:55.382246 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.382446 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:55.382569 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:55.382613 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:55.385694 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.385779 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:55.387899 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.387979 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:55.388077 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:55.391499 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:55.394537 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.394624 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:55.394875 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.394958 139878876468288 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0122 03:14:55.395051 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:55.395097 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:55.395131 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:55.395177 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.397123 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:55.406044 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.406463 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:55.408658 139878876468288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:14:55.419948 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:55.420005 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:55.420046 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:55.420084 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.420157 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.420468 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.420553 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.420866 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.421507 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.423655 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.423997 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.424084 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:55.424119 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:55.424166 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.424393 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:55.424501 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:55.424551 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:55.427603 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.427701 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:55.429768 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.429857 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:55.429952 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:55.433368 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:55.436590 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.436674 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:55.436920 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.437001 139878876468288 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0122 03:14:55.437097 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:55.437137 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:55.437169 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:55.437214 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.439171 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:55.448110 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.448509 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:55.450745 139878876468288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:14:55.461953 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:55.462026 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:55.462066 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:55.462103 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.462176 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.462478 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.462554 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.462880 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.463535 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.465652 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.465981 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.466068 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:55.466104 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:55.466150 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.466352 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:55.466461 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:55.466514 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:55.469622 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.469704 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:55.471784 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.471861 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:55.471956 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:55.475343 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:55.478368 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.478452 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:55.478702 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.478784 139878876468288 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0122 03:14:55.478877 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:55.478920 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:55.478952 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:55.478998 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.480936 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:55.490015 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.490425 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:55.492602 139878876468288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:14:55.504059 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:55.504139 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:55.504179 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:55.504216 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.504291 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.504601 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.504675 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.505002 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.505752 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.507976 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.508306 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.508392 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:55.508428 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:55.508488 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.508702 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:55.508798 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:55.508843 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:55.511946 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.512042 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:55.514088 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.514180 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:55.514291 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:55.517922 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:55.520999 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.521097 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:55.521353 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.521447 139878876468288 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0122 03:14:55.521542 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:55.521583 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:55.521615 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:55.521663 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.523620 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:55.532467 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.532891 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:55.535123 139878876468288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:14:55.546716 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:55.546800 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:55.546842 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:55.546896 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.546983 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.547280 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.547352 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.547659 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.548300 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.550651 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.550982 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.551069 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:55.551104 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:55.551151 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.551355 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:55.551448 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:55.551488 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:55.554585 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.554669 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:55.556725 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.556803 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:55.556891 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:55.560244 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:55.563277 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.563363 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:55.563602 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.563684 139878876468288 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0122 03:14:55.563774 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:55.563813 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:55.563845 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:55.563891 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.565823 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:55.574875 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.575297 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:55.577494 139878876468288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:14:55.589257 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:55.589327 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:55.589377 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:55.589417 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.589488 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.589788 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.589861 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.590172 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.590837 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.592996 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.593299 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.593372 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:55.593408 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:55.593456 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.593630 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:55.593723 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:55.593770 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:55.596888 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.596970 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:55.599054 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.599129 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:55.599223 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:55.603574 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:55.607436 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.607568 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:55.607854 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.607941 139878876468288 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0122 03:14:55.608064 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:55.608108 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:55.608141 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:55.608207 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.610245 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:55.619397 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.619852 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:55.622804 139878876468288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:14:55.634973 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:55.635058 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:55.635099 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:55.635145 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.635223 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.635524 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.635595 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.635903 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.636551 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.639371 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.639752 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.639836 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:55.639873 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:55.639924 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.640121 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:55.640224 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:55.640269 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:55.643554 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.643650 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:55.645756 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.645833 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:55.645921 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:55.649432 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:55.652498 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.652598 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:55.652850 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.652936 139878876468288 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0122 03:14:55.653025 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:55.653072 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:55.653104 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:55.653151 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.655518 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:55.665009 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.665476 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:55.667717 139878876468288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:14:55.679949 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:55.680034 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:55.680088 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:55.680147 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.680260 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.680612 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.680692 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.681084 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.682054 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.684306 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.684647 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.684729 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:55.684782 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:55.684854 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.685048 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:55.685150 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:55.685194 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:55.688872 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.689017 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:55.691637 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.691753 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:55.691856 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:55.695340 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:55.698430 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.698538 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:55.698798 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.698882 139878876468288 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0122 03:14:55.698979 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:55.699020 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:55.699052 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:55.699098 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.701077 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:55.710965 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.711405 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:55.713640 139878876468288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:14:55.725675 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:55.725759 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:55.725800 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:55.725836 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.725924 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.726241 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.726312 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.726689 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.727333 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.729743 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.730069 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.730145 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:55.730181 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:55.730231 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.730409 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:55.730505 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:55.730557 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:55.838682 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.838852 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:55.841335 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.841452 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:55.841557 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:55.845351 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:55.848572 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.848687 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:55.849017 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.849106 139878876468288 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0122 03:14:55.849199 139878876468288 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:14:55.849238 139878876468288 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:14:55.849271 139878876468288 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:14:55.849317 139878876468288 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.851365 139878876468288 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:14:55.866785 139878876468288 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.867255 139878876468288 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:14:55.870579 139878876468288 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:14:55.898551 139878876468288 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:14:55.898670 139878876468288 attention.py:418] Single window, no scan.
I0122 03:14:55.898743 139878876468288 transformer_layer.py:389] tlayer: self-attention.
I0122 03:14:55.898816 139878876468288 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.898935 139878876468288 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.899490 139878876468288 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.899600 139878876468288 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.900285 139878876468288 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.901520 139878876468288 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.904985 139878876468288 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.905655 139878876468288 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.905790 139878876468288 transformer_layer.py:468] tlayer: End windows.
I0122 03:14:55.905854 139878876468288 transformer_layer.py:472] tlayer: final FFN.
I0122 03:14:55.905947 139878876468288 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.906308 139878876468288 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:14:55.906487 139878876468288 nn_components.py:325] mlp: activation = None
I0122 03:14:55.906563 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:55.914984 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.915158 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:55.919317 139878876468288 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.919486 139878876468288 transformer_base.py:443] tbase: final FFN
I0122 03:14:55.919651 139878876468288 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:14:55.928138 139878876468288 nn_components.py:329] mlp: final activation = None
I0122 03:14:55.934741 139878876468288 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.934942 139878876468288 nn_components.py:261] mlp: residual
I0122 03:14:55.935476 139878876468288 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:14:55.935621 139878876468288 decoder_stack.py:344] dstack: Final layernorm.
I0122 03:14:55.940743 139878876468288 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:10.940976 139878876468288 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/optim/base.py:49: DeprecationWarning: Use `optax` instead of `flax.optim`. Refer to the update guide https://flax.readthedocs.io/en/latest/howtos/optax_update_guide.html for detailed instructions.
  warnings.warn(
I0122 03:15:11.251140 139878876468288 training_loop.py:409] No working directory specified.
I0122 03:15:11.251240 139878876468288 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0122 03:15:11.251667 139878876468288 checkpoints.py:425] Found no checkpoint files in ag_ckpt_vocab with prefix checkpoint_
I0122 03:15:11.251758 139878876468288 training_loop.py:447] Only restoring trainable parameters.
I0122 03:15:11.252282 139878876468288 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0122 03:15:11.252359 139878876468288 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0122 03:15:11.252427 139878876468288 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:15:11.252491 139878876468288 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:15:11.252572 139878876468288 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.252640 139878876468288 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:11.252707 139878876468288 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.252773 139878876468288 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.252838 139878876468288 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:15:11.252903 139878876468288 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:15:11.252969 139878876468288 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.253034 139878876468288 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:11.253100 139878876468288 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:15:11.253165 139878876468288 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:15:11.253230 139878876468288 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.253295 139878876468288 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:11.253360 139878876468288 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.253425 139878876468288 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.253491 139878876468288 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:15:11.253556 139878876468288 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:15:11.253621 139878876468288 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.253687 139878876468288 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:11.253751 139878876468288 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:15:11.253816 139878876468288 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:15:11.253881 139878876468288 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.253946 139878876468288 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:11.254011 139878876468288 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.254077 139878876468288 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.254142 139878876468288 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:15:11.254207 139878876468288 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:15:11.254272 139878876468288 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.254343 139878876468288 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:11.254408 139878876468288 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:15:11.254473 139878876468288 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:15:11.254546 139878876468288 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.254612 139878876468288 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:11.254678 139878876468288 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.254743 139878876468288 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.254808 139878876468288 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:15:11.254874 139878876468288 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:15:11.254939 139878876468288 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.255004 139878876468288 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:11.255069 139878876468288 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:15:11.255135 139878876468288 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:15:11.255200 139878876468288 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.255265 139878876468288 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:11.255330 139878876468288 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.255395 139878876468288 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.255461 139878876468288 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:15:11.255526 139878876468288 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:15:11.255591 139878876468288 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.255656 139878876468288 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:11.255722 139878876468288 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:15:11.255787 139878876468288 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:15:11.255852 139878876468288 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.255918 139878876468288 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:11.255983 139878876468288 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.256053 139878876468288 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.256119 139878876468288 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:15:11.256185 139878876468288 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:15:11.256251 139878876468288 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.256316 139878876468288 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:11.256381 139878876468288 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:15:11.256446 139878876468288 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:15:11.256511 139878876468288 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.256576 139878876468288 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:11.256642 139878876468288 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.256707 139878876468288 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.256772 139878876468288 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:15:11.256837 139878876468288 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:15:11.256903 139878876468288 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.256968 139878876468288 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:11.257034 139878876468288 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:15:11.257099 139878876468288 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:15:11.257164 139878876468288 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.257229 139878876468288 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:11.257295 139878876468288 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.257360 139878876468288 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.257426 139878876468288 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:15:11.257491 139878876468288 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:15:11.257556 139878876468288 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.257622 139878876468288 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:11.257687 139878876468288 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:15:11.257753 139878876468288 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:15:11.257823 139878876468288 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.257889 139878876468288 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:11.257954 139878876468288 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.258019 139878876468288 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.258085 139878876468288 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:15:11.258150 139878876468288 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:15:11.258215 139878876468288 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.258280 139878876468288 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:11.258346 139878876468288 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:15:11.258410 139878876468288 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:15:11.258476 139878876468288 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.258547 139878876468288 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:11.258613 139878876468288 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.258679 139878876468288 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.258744 139878876468288 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:15:11.258809 139878876468288 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:15:11.258875 139878876468288 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.258940 139878876468288 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:11.259006 139878876468288 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:15:11.259070 139878876468288 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:15:11.259135 139878876468288 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.259201 139878876468288 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:11.259266 139878876468288 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.259331 139878876468288 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.259397 139878876468288 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:15:11.259462 139878876468288 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:15:11.259527 139878876468288 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.259597 139878876468288 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:11.259662 139878876468288 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:15:11.259728 139878876468288 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:15:11.259793 139878876468288 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.259859 139878876468288 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:11.259924 139878876468288 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.259989 139878876468288 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.260054 139878876468288 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:15:11.260119 139878876468288 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:15:11.260185 139878876468288 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:11.260249 139878876468288 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:11.260297 139878876468288 training_loop.py:725] Total parameters: 152072288
I0122 03:15:11.260519 139878876468288 training_loop.py:739] Total state size: 0
I0122 03:15:11.372468 139878876468288 training_loop.py:492] Training loop: creating task for mode beam_search
I0122 03:15:11.372658 139878876468288 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0122 03:15:11.373026 139878876468288 training_loop.py:652] Compiling mode beam_search with jit.
I0122 03:15:11.373281 139878876468288 training_loop.py:89] registering functions: dict_keys([])
I0122 03:15:11.377166 139878876468288 graph.py:498] translated_imo_2000_p1
I0122 03:15:11.377264 139878876468288 graph.py:499] a b = segment a b; c = on_tline c a a b; d = on_tline d b b a; e = on_circle e c a, on_circle e d b; f = on_circle f c a, on_circle f d b; g = on_pline g e a b, on_circle g c a; h = on_pline h e a b, on_circle h d b; i = on_line i a g, on_line i b h; j = on_line j a f, on_line j g h; k = on_line k b f, on_line k g h ? cong i j i k
I0122 03:15:12.242030 139878876468288 ddar.py:60] Depth 1/1000 time = 0.8251144886016846
I0122 03:15:14.795369 139878876468288 ddar.py:60] Depth 2/1000 time = 2.553203582763672
I0122 03:15:17.896388 139878876468288 ddar.py:60] Depth 3/1000 time = 3.1008830070495605
I0122 03:15:22.450743 139878876468288 ddar.py:60] Depth 4/1000 time = 4.55422568321228
I0122 03:15:29.125128 139878876468288 ddar.py:60] Depth 5/1000 time = 6.67423677444458
I0122 03:15:29.159785 139878876468288 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H I J K : Points
AC ⟂ AB [00]
BA ⟂ DB [01]
CE = CA [02]
DE = DB [03]
DF = DB [04]
CF = CA [05]
GE ∥ AB [06]
CG = CA [07]
∠GAF = ∠GAF [08]
HE ∥ AB [09]
DH = DB [10]
∠HBF = ∠HBF [11]
G,A,I are collinear [12]
I,B,H are collinear [13]
G,J,H are collinear [14]
F,A,J are collinear [15]
GJ:EJ = GJ:EJ [16]
F,K,B are collinear [17]
G,K,H are collinear [18]

 * Auxiliary Constructions:
: Points


 * Proof steps:
001. EH ∥ AB [09] & EG ∥ AB [06] ⇒  EG ∥ EH [19]
002. EG ∥ EH [19] ⇒  G,E,H are collinear [20]
003. CG = CA [07] & CE = CA [02] ⇒  C is the circumcenter of \Delta AGE [21]
004. C is the circumcenter of \Delta AGE [21] & AC ⟂ AB [00] ⇒  ∠BAG = ∠AEG [22]
005. G,E,H are collinear [20] & G,J,H are collinear [14] & G,A,I are collinear [12] & ∠BAG = ∠AEG [22] & EG ∥ AB [06] ⇒  ∠JGI = ∠EAB [23]
006. DH = DB [10] & DE = DB [03] ⇒  D is the circumcenter of \Delta BHE [24]
007. D is the circumcenter of \Delta BHE [24] & DB ⟂ BA [01] ⇒  ∠ABH = ∠BEH [25]
008. DE = DB [03] & DH = DB [10] & DF = DB [04] ⇒  F,E,B,H are concyclic [26]
009. F,E,B,H are concyclic [26] ⇒  ∠FEH = ∠FBH [27]
010. F,E,B,H are concyclic [26] ⇒  ∠EFB = ∠EHB [28]
011. CE = CA [02] & CG = CA [07] & CF = CA [05] ⇒  F,A,E,G are concyclic [29]
012. F,A,E,G are concyclic [29] ⇒  ∠FAG = ∠FEG [30]
013. G,A,I are collinear [12] & I,B,H are collinear [13] & ∠FEH = ∠FBH [27] & EH ∥ AB [09] & ∠FAG = ∠FEG [30] & EG ∥ AB [06] ⇒  ∠FAI = ∠FBI [31]
014. ∠FAI = ∠FBI [31] ⇒  F,A,I,B are concyclic [32]
015. F,A,I,B are concyclic [32] ⇒  ∠FAB = ∠FIB [33]
016. F,A,I,B are concyclic [32] ⇒  ∠FIA = ∠FBA [34]
017. CF = CA [05] & CE = CA [02] ⇒  C is the circumcenter of \Delta AFE [35]
018. C is the circumcenter of \Delta AFE [35] & AC ⟂ AB [00] ⇒  ∠FAB = ∠FEA [36]
019. C is the circumcenter of \Delta AFE [35] & AC ⟂ AB [00] ⇒  ∠EAB = ∠EFA [37]
020. F,A,J are collinear [15] & G,E,H are collinear [20] & G,J,H are collinear [14] & ∠FAB = ∠FEA [36] & AB ∥ EG [06] ⇒  ∠AJE = ∠FEA [38]
021. G,E,H are collinear [20] & G,J,H are collinear [14] & ∠EAB = ∠EFA [37] & AB ∥ EG [06] ⇒  ∠AEJ = ∠EFA [39]
022. ∠AJE = ∠FEA [38] & ∠AEJ = ∠EFA [39] (Similar Triangles)⇒  AJ:AE = AE:FA [40]
023. G,A,I are collinear [12] & ∠BAG = ∠AEG [22] & EG ∥ AB [06] ⇒  ∠IAB = ∠BAE [41]
024. I,B,H are collinear [13] & ∠ABH = ∠BEH [25] & EH ∥ AB [09] ⇒  ∠IBA = ∠ABE [42]
025. ∠IAB = ∠BAE [41] & ∠IBA = ∠ABE [42] (Similar Triangles)⇒  AI = AE [43]
026. ∠IAB = ∠BAE [41] & ∠IBA = ∠ABE [42] (Similar Triangles)⇒  BI = BE [44]
027. AE:FA = AJ:AE [40] & AI = AE [43] ⇒  AI:AF = AJ:AI [45]
028. G,A,I are collinear [12] & F,A,J are collinear [15] & ∠GAF = ∠GAF [08] ⇒  ∠IAF = ∠IAJ [46]
029. AI:AF = AJ:AI [45] & ∠IAF = ∠IAJ [46] (Similar Triangles)⇒  ∠AIF = ∠IJA [47]
030. G,A,I are collinear [12] & ∠ABH = ∠BEH [25] & EH ∥ AB [09] & ∠FAB = ∠FIB [33] & I,B,H are collinear [13] & ∠AIF = ∠IJA [47] & F,A,J are collinear [15] ⇒  ∠JIG = ∠EBA [48]
031. ∠JGI = ∠EAB [23] & ∠JIG = ∠EBA [48] (Similar Triangles)⇒  JG:JI = EA:EB [49]
032. DF = DB [04] & DE = DB [03] ⇒  D is the circumcenter of \Delta BFE [50]
033. D is the circumcenter of \Delta BFE [50] & DB ⟂ BA [01] ⇒  ∠FBA = ∠FEB [51]
034. D is the circumcenter of \Delta BFE [50] & DB ⟂ BA [01] ⇒  ∠EBA = ∠EFB [52]
035. F,K,B are collinear [17] & G,E,H are collinear [20] & G,K,H are collinear [18] & ∠FBA = ∠FEB [51] & AB ∥ EG [06] ⇒  ∠BKE = ∠FEB [53]
036. G,E,H are collinear [20] & G,K,H are collinear [18] & ∠EBA = ∠EFB [52] & AB ∥ EG [06] ⇒  ∠BEK = ∠EFB [54]
037. ∠BKE = ∠FEB [53] & ∠BEK = ∠EFB [54] (Similar Triangles)⇒  BK:BE = BE:BF [55]
038. EB:KB = FB:EB [55] & IB = EB [44] ⇒  BI:BK = BF:BI [56]
039. I,B,H are collinear [13] & F,K,B are collinear [17] & ∠HBF = ∠HBF [11] ⇒  ∠IBK = ∠IBF [57]
040. BI:BK = BF:BI [56] & ∠IBK = ∠IBF [57] (Similar Triangles)⇒  ∠BIK = ∠IFB [58]
041. F,K,B are collinear [17] & F,A,J are collinear [15] & G,E,H are collinear [20] & G,J,H are collinear [14] & ∠FAB = ∠FIB [33] & I,B,H are collinear [13] & ∠BIK = ∠IFB [58] & AB ∥ EG [06] ⇒  ∠BKI = ∠AJG [59]
042. B,I,H are collinear [13] & G,E,H are collinear [20] & G,J,H are collinear [14] & ∠FIA = ∠FBA [34] & G,A,I are collinear [12] & ∠BIK = ∠IFB [58] & AB ∥ EG [06] ⇒  ∠BIK = ∠AGJ [60]
043. ∠BKI = ∠AJG [59] & ∠BIK = ∠AGJ [60] (Similar Triangles)⇒  IB:GA = IK:GJ [61]
044. G,E,H are collinear [20] & EG ∥ AB [06] ⇒  HG ∥ BA [62]
045. HG ∥ BA [62] & I,B,H are collinear [13] & G,A,I are collinear [12] ⇒  BI:BH = AI:AG [63]
046. DF = DB [04] & DH = DB [10] ⇒  D is the circumcenter of \Delta BFH [64]
047. D is the circumcenter of \Delta BFH [64] & DB ⟂ BA [01] ⇒  ∠ABH = ∠BFH [65]
048. ∠ABH = ∠BFH [65] & ∠EFB = ∠EHB [28] & EH ∥ AB [09] ⇒  ∠EFB = ∠BFH [66]
049. F,E,B,H are concyclic [26] & ∠EFB = ∠BFH [66] ⇒  EB = BH [67]
050. JG:JI = EA:EB [49] & AI = AE [43] & BI = BE [44] & IB:GA = IK:GJ [61] & BI:BH = AI:AG [63] & EB = BH [67] ⇒  KI:GJ = JI:GJ [68]
051. KI:GJ = JI:GJ [68] & GJ:EJ = GJ:EJ [16] ⇒  KI = JI
==========================

I0122 03:15:29.159987 139878876468288 alphageometry.py:195] Solution written to ./output/imo_2000_p1_solution.txt.
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
2024-01-22 03:15:33.227130: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
I0122 03:15:34.758697 140223234692160 inference_utils.py:69] Parsing gin configuration.
I0122 03:15:34.758808 140223234692160 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0122 03:15:34.759009 140223234692160 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0122 03:15:34.759057 140223234692160 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0122 03:15:34.759092 140223234692160 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0122 03:15:34.759131 140223234692160 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0122 03:15:34.759181 140223234692160 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0122 03:15:34.759226 140223234692160 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0122 03:15:34.759275 140223234692160 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0122 03:15:34.759319 140223234692160 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=2
I0122 03:15:34.759367 140223234692160 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0122 03:15:34.759416 140223234692160 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0122 03:15:34.759488 140223234692160 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0122 03:15:34.759673 140223234692160 resource_reader.py:55] Path not found: base_htrans.gin
I0122 03:15:34.759881 140223234692160 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0122 03:15:34.760050 140223234692160 resource_reader.py:55] Path not found: trainer_configuration.gin
I0122 03:15:34.765259 140223234692160 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0122 03:15:34.765425 140223234692160 resource_reader.py:55] Path not found: size/medium_150M.gin
I0122 03:15:34.765783 140223234692160 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0122 03:15:34.765926 140223234692160 resource_reader.py:55] Path not found: options/positions_t5.gin
I0122 03:15:34.766364 140223234692160 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0122 03:15:34.766548 140223234692160 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0122 03:15:34.767295 140223234692160 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0122 03:15:34.767443 140223234692160 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0122 03:15:34.772336 140223234692160 training_loop.py:334] ==== Training loop: initializing model ====
I0122 03:15:34.774677 140223234692160 xla_bridge.py:166] Remote TPU is not linked into jax; skipping remote TPU.
I0122 03:15:34.774747 140223234692160 xla_bridge.py:413] Unable to initialize backend 'tpu_driver': Could not initialize backend 'tpu_driver'
I0122 03:15:34.774822 140223234692160 xla_bridge.py:413] Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
I0122 03:15:34.774890 140223234692160 xla_bridge.py:413] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
I0122 03:15:34.775309 140223234692160 xla_bridge.py:413] Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.
I0122 03:15:34.775388 140223234692160 xla_bridge.py:413] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
W0122 03:15:34.775449 140223234692160 xla_bridge.py:420] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0122 03:15:34.775521 140223234692160 training_loop.py:335] Process 0 of 1
I0122 03:15:34.775577 140223234692160 training_loop.py:336] Local device count = 1
I0122 03:15:34.775639 140223234692160 training_loop.py:337] Number of replicas = 1
I0122 03:15:34.775690 140223234692160 training_loop.py:339] Using random number seed 42
I0122 03:15:34.961514 140223234692160 training_loop.py:359] Initializing the model.
I0122 03:15:35.060201 140223234692160 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.060533 140223234692160 decoder_stack.py:316] dstack: scanning over 1 windows.
I0122 03:15:35.060650 140223234692160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:15:35.060738 140223234692160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:15:35.060820 140223234692160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:15:35.060899 140223234692160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:15:35.060974 140223234692160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:15:35.061052 140223234692160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:15:35.061130 140223234692160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:15:35.061206 140223234692160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:15:35.061282 140223234692160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:15:35.061360 140223234692160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:15:35.061438 140223234692160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:15:35.061527 140223234692160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:15:35.061595 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:35.061662 140223234692160 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0122 03:15:35.061843 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:35.061908 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:35.061967 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:35.064213 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.073119 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:35.098132 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.098839 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:35.104522 140223234692160 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:15:35.124396 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:35.124485 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:35.124530 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:35.124572 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.124658 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.125571 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.125649 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.126359 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.128912 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.135292 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.136270 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.136367 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:35.136425 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:35.136489 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.136714 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:35.137058 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:35.137118 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.141578 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.141681 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.144436 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.144526 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:35.144830 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:35.162613 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.174129 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.174250 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.174512 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.174608 140223234692160 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0122 03:15:35.174702 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:35.174743 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:35.174778 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:35.176212 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.178368 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:35.186895 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.187303 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:35.189564 140223234692160 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:15:35.194162 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:35.194222 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:35.194263 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:35.194303 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.194383 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.194738 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.194811 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.195127 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.195891 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.198051 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.198379 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.198454 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:35.198491 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:35.198547 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.198736 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:35.198976 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:35.199024 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.201906 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.201992 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.204263 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.204340 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:35.204627 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:35.207899 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.210782 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.210866 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.211116 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.211206 140223234692160 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0122 03:15:35.211306 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:35.211349 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:35.211382 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:35.213056 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.216758 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:35.231557 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.232214 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:35.235556 140223234692160 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:15:35.240693 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:35.240750 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:35.240790 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:35.240831 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.240902 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.241244 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.241319 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.241630 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.242385 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.244540 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.244848 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.244923 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:35.244960 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:35.245008 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.245178 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:35.245415 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:35.245461 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.248346 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.248429 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.250681 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.250758 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:35.251038 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:35.254279 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.257459 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.257545 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.257789 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.257869 140223234692160 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0122 03:15:35.257968 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:35.258009 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:35.258042 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:35.345829 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.349177 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:35.361944 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.362620 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:35.365836 140223234692160 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:15:35.371515 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:35.371585 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:35.371625 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:35.371662 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.371739 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.372044 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.372117 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.372433 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.373233 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.375389 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.375725 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.375814 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:35.375850 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:35.375897 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.376100 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:35.376359 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:35.376406 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.379285 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.379367 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.381602 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.381708 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:35.382001 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:35.385441 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.388494 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.388622 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.388904 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.389003 140223234692160 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0122 03:15:35.389120 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:35.389169 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:35.389233 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:35.391327 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.394623 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:35.404642 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.405108 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:35.407390 140223234692160 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:15:35.411656 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:35.411716 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:35.411771 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:35.411827 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.411908 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.412277 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.412358 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.412697 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.413472 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.415698 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.416029 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.416105 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:35.416154 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:35.416225 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.416422 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:35.416695 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:35.416748 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.419663 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.419748 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.422050 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.422130 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:35.422430 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:35.425672 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.428583 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.428673 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.428936 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.429019 140223234692160 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0122 03:15:35.429131 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:35.429176 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:35.429217 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:35.430599 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.432749 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:35.441227 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.441654 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:35.443959 140223234692160 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:15:35.448170 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:35.448230 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:35.448283 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:35.448342 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.448419 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.448751 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.448831 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.449167 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.449949 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.452111 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.452442 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.452518 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:35.452570 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:35.452641 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.452838 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:35.453089 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:35.453143 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.456663 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.456800 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.459337 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.459459 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:35.459812 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:35.463959 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.466957 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.467053 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.467322 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.467406 140223234692160 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0122 03:15:35.467517 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:35.467561 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:35.467602 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:35.468988 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.471395 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:35.480504 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.480973 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:35.483969 140223234692160 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:15:35.488366 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:35.488442 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:35.488497 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:35.488554 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.488640 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.488977 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.489057 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.489396 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.490233 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.492445 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.492813 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.492891 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:35.492939 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:35.493000 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.493245 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:35.493616 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:35.493679 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.496950 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.497087 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.500259 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.500391 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:35.500752 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:35.504164 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.507118 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.507225 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.507501 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.507596 140223234692160 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0122 03:15:35.507717 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:35.507767 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:35.507822 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:35.509207 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.512863 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:35.524036 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.524478 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:35.526951 140223234692160 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:15:35.533546 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:35.533651 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:35.533720 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:35.533780 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.533888 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.534336 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.534435 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.534930 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.535788 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.539081 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.539443 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.539528 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:35.539585 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:35.539668 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.539920 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:35.540271 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:35.540331 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.544169 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.544275 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.546660 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.546759 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:35.547096 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:35.550373 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.554325 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.554444 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.554815 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.554923 140223234692160 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0122 03:15:35.555058 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:35.555119 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:35.555174 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:35.557188 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.559765 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:35.568205 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.568682 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:35.570944 140223234692160 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:15:35.575450 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:35.575516 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:35.575579 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:35.575642 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.575723 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.576126 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.576219 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.576660 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.577577 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.579710 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.580032 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.580113 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:35.580151 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:35.580214 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.580430 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:35.580760 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:35.580821 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.584193 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.584281 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.586581 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.586664 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:35.586954 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:35.590197 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.593570 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.593681 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.593981 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.594074 140223234692160 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0122 03:15:35.594174 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:35.594223 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:35.594260 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:35.596210 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.599066 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:35.607458 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.607900 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:35.610127 140223234692160 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:15:35.614326 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:35.614406 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:35.614464 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:35.614532 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.614621 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.615030 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.615135 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.615584 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.616672 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.618875 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.619193 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.619273 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:35.619313 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:35.619371 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.619580 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:35.619911 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:35.619971 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.623632 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.623724 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.625967 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.626050 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:35.626339 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:35.630756 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.634268 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.634406 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.634703 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.634796 140223234692160 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0122 03:15:35.634897 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:35.634946 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:35.634983 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:35.636967 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.639274 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:35.647831 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.648258 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:35.650532 140223234692160 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:15:35.654829 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:35.654895 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:35.654940 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:35.654992 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.655085 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.655505 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.655599 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.656044 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.657129 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.659326 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.659650 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.659731 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:35.659771 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:35.659836 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.660053 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:35.660383 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:35.660444 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.663929 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.664018 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.666284 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.666370 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:35.666719 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:35.669946 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.672840 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.672930 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.673185 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.673272 140223234692160 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0122 03:15:35.673376 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:35.673424 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:35.673460 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:35.675397 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.678143 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:35.686534 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.686949 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:35.689502 140223234692160 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:15:35.694164 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:35.694265 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:35.694323 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:35.694377 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.694463 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.694990 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.695101 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.695552 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.696654 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.698838 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.699154 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.699235 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:35.699274 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:35.699338 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.699557 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:35.699885 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:35.699946 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.703629 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.703719 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.705957 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.706041 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:35.706333 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:35.709622 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.712506 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.712602 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.712853 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.713097 140223234692160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:15:35.713159 140223234692160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:15:35.713210 140223234692160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:15:35.713260 140223234692160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:15:35.713320 140223234692160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:15:35.713382 140223234692160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:15:35.713444 140223234692160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:15:35.713503 140223234692160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:15:35.713564 140223234692160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:15:35.713625 140223234692160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:15:35.713695 140223234692160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:15:35.713757 140223234692160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:15:35.713813 140223234692160 decoder_stack.py:344] dstack: Final layernorm.
I0122 03:15:35.718279 140223234692160 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:15:35.783792 140223234692160 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.783927 140223234692160 decoder_stack.py:333] dstack: autoregressive generator.
I0122 03:15:35.783985 140223234692160 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0122 03:15:35.784102 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:35.784151 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:35.784205 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:35.784282 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.787318 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:35.796832 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.797312 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:35.799700 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:15:35.816724 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:35.816827 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:35.816899 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:35.816953 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.817044 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.818373 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.818472 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.819727 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.822737 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.828473 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.829420 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.829513 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:35.829552 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:35.829617 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.829841 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:35.829973 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:35.830030 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.834686 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.834777 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.836857 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.836950 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:35.837049 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:35.840829 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.843878 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.843986 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.844244 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.844360 140223234692160 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0122 03:15:35.844487 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:35.844541 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:35.844602 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:35.844685 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.847705 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:35.858508 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.859013 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:35.861563 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:15:35.873624 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:35.873710 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:35.873753 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:35.873793 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.873866 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.874175 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.874250 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.874571 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.875223 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.877432 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.877752 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.877828 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:35.877865 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:35.877917 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.878093 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:35.878189 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:35.878230 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.881382 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.881468 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.883570 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.883648 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:35.883747 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:35.887137 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.890153 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.890238 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.890476 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.890566 140223234692160 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0122 03:15:35.890676 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:35.890726 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:35.890779 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:35.890851 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.892833 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:35.901909 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.902330 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:35.904561 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:15:35.916078 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:35.916164 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:35.916226 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:35.916289 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.916371 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.916712 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.916806 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.917146 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.917813 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.920024 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.920344 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.920427 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:35.920482 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:35.920556 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.920757 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:35.920868 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:35.920918 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.924045 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.924136 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.926235 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.926319 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:35.926451 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:35.929853 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.932950 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.933062 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.933343 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.933431 140223234692160 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0122 03:15:35.933524 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:35.933565 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:35.933598 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:35.933643 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.935629 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:35.944650 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.945050 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:35.947271 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:15:35.959052 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:35.959135 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:35.959178 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:35.959229 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.959365 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.959711 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.959805 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.960146 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.960816 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.963026 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.963372 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.963469 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:35.963523 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:35.963597 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.963835 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:35.963960 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:35.964010 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.967184 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.967277 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.969397 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.969482 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:35.969599 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:35.972988 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:35.976026 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.976120 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:35.976382 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.976476 140223234692160 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0122 03:15:35.976589 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:35.976638 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:35.976691 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:35.976761 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.979514 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:35.988512 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:35.988939 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:35.991153 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:15:36.002696 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:36.002764 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:36.002826 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:36.002887 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.002969 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.003297 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.003378 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.003715 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.004385 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.006588 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.006928 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.007014 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:36.007072 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:36.007135 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.007344 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:36.007467 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:36.007520 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.010946 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.011083 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.013339 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.013476 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:36.013605 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:36.017453 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.020648 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.020770 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.021043 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.021142 140223234692160 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0122 03:15:36.021257 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:36.021308 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:36.021359 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:36.021434 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.024297 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:36.033923 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.034391 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:36.036643 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:15:36.048816 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:36.048913 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:36.048967 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:36.049033 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.049118 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.049475 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.049551 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.049915 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.050643 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.052871 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.053214 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.053294 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:36.053335 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:36.053405 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.053632 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:36.053746 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:36.053796 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.057246 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.057373 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.059525 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.059604 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:36.059711 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:36.063431 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.066750 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.066897 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.067185 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.067274 140223234692160 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0122 03:15:36.067384 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:36.067429 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:36.067470 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:36.067532 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.069584 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:36.078850 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.079347 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:36.081563 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:15:36.093211 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:36.093295 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:36.093351 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:36.093409 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.093484 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.093818 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.093902 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.094241 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.095132 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.097319 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.097671 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.097768 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:36.097823 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:36.097898 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.098170 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:36.098303 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:36.098359 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.101544 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.101635 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.103744 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.103829 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:36.103945 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:36.107332 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.110382 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.110478 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.110746 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.110832 140223234692160 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0122 03:15:36.110924 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:36.110966 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:36.110999 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:36.111046 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.112984 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:36.121889 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.122286 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:36.124483 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:15:36.136585 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:36.136669 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:36.136711 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:36.136750 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.136823 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.137123 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.137197 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.137687 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.138379 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.140574 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.140883 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.140958 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:36.140994 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:36.141045 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.141221 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:36.141317 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:36.141359 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.144549 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.144637 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.146753 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.146832 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:36.146922 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:36.150318 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.153332 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.153420 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.153669 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.153770 140223234692160 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0122 03:15:36.153872 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:36.153920 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:36.153956 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:36.154013 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.156842 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:36.166303 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.166734 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:36.168939 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:15:36.180528 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:36.180612 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:36.180668 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:36.180722 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.180808 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.181233 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.181339 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.181795 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.182693 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.184848 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.185184 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.185280 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:36.185328 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:36.185401 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.185694 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:36.185837 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:36.185920 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.189223 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.189328 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.191814 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.191910 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:36.192034 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:36.195642 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.198674 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.198766 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.199010 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.199098 140223234692160 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0122 03:15:36.199206 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:36.199254 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:36.199290 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:36.199345 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.202095 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:36.212538 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.212945 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:36.215165 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:15:36.226683 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:36.226764 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:36.226809 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:36.226860 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.226938 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.227346 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.227454 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.227908 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.228842 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.231102 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.231440 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.231525 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:36.231564 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:36.231627 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.231849 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:36.231980 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:36.232037 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.236751 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.236852 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.238949 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.239031 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:36.239129 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:36.242500 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.245532 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.245625 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.245955 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.246055 140223234692160 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0122 03:15:36.246155 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:36.246211 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:36.246248 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:36.246312 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.248721 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:36.259479 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.260018 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:36.262313 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:15:36.274312 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:36.274404 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:36.274462 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:36.274535 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.274630 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.275053 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.275143 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.275582 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.276523 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.279811 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.280229 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.280313 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:36.280350 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:36.280415 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.280619 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:36.280732 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:36.280775 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.284617 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.284743 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.286903 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.287000 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:36.287121 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:36.290600 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.293685 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.293818 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.294212 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.294334 140223234692160 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0122 03:15:36.294476 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:36.294539 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:36.294605 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:36.294687 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.297713 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:36.307415 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.307874 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:36.310213 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:15:36.322099 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:36.322201 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:36.322267 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:36.322329 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.322426 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.322811 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.322894 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.323227 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.323885 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.326069 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.326383 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.326460 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:36.326508 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:36.326579 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.326776 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:36.326882 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:36.326925 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.330063 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.330149 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.332235 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.332312 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:36.332410 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:36.335786 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.338830 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.338929 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.339189 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.339290 140223234692160 decoder_stack.py:344] dstack: Final layernorm.
I0122 03:15:36.341728 140223234692160 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:15:36.410543 140223234692160 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.410696 140223234692160 decoder_stack.py:333] dstack: autoregressive generator.
I0122 03:15:36.410746 140223234692160 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0122 03:15:36.410866 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:36.410923 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:36.410985 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:36.411059 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.413509 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:36.422374 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.422825 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:36.424995 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:15:36.436280 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:36.436358 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:36.436416 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:36.436479 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.436579 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.436910 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.436991 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.437331 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.437986 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.440157 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.440476 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.440552 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:36.440601 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:36.440675 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.440871 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:36.440983 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:36.441033 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.444097 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.444188 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.446292 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.446376 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:36.446496 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:36.449858 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.452887 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.452973 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.453231 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.453326 140223234692160 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0122 03:15:36.453450 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:36.453495 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:36.453536 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:36.453606 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.455579 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:36.464652 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.465073 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:36.467273 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:15:36.480461 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:36.480562 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:36.480633 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:36.480699 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.480802 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.481287 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.481400 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.481918 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.482822 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.485005 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.485319 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.485395 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:36.485433 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:36.485491 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.485723 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:36.485851 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:36.485905 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.489386 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.489492 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.491625 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.491707 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:36.491811 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:36.495439 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.498587 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.498690 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.498949 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.499044 140223234692160 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0122 03:15:36.499149 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:36.499191 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:36.499225 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:36.499282 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.502022 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:36.511147 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.511610 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:36.513866 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:15:36.525286 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:36.525362 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:36.525405 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:36.525453 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.525538 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.525931 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.526022 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.526467 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.527454 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.530888 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.531225 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.531302 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:36.531338 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:36.531401 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.531604 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:36.531703 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:36.531760 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.535796 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.535904 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.538036 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.538121 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:36.538235 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:36.543219 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.546372 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.546489 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.546762 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.546850 140223234692160 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0122 03:15:36.546966 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:36.547009 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:36.547043 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:36.547089 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.549093 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:36.558139 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.558583 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:36.560845 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:15:36.572317 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:36.572392 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:36.572442 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:36.572507 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.572596 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.572932 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.573003 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.573350 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.574007 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.576219 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.576641 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.576721 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:36.576768 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:36.576838 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.577041 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:36.577156 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:36.577207 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.581950 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.582070 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.584391 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.584480 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:36.584591 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:36.588016 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.591369 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.591472 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.591743 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.591829 140223234692160 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0122 03:15:36.591939 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:36.591990 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:36.592031 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:36.592103 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.594144 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:36.603290 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.603742 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:36.605957 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:15:36.617406 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:36.617485 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:36.617540 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:36.617595 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.617684 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.618016 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.618088 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.618432 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.619102 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.621257 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.621583 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.621663 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:36.621716 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:36.621791 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.621986 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:36.622096 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:36.622145 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.626723 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.626838 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.629037 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.629137 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:36.629259 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:36.632785 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.635842 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.635947 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.636211 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.636307 140223234692160 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0122 03:15:36.636416 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:36.636464 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:36.636525 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:36.636596 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.639183 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:36.648749 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.649212 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:36.651460 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:15:36.663203 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:36.663293 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:36.663357 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:36.663418 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.663505 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.663836 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.663915 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.664258 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.664928 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.667179 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.667496 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.667577 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:36.667630 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:36.667706 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.667901 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:36.668024 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:36.668067 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.671236 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.671334 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.673423 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.673499 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:36.673589 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:36.677238 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.680620 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.680746 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.681017 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.681114 140223234692160 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0122 03:15:36.681225 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:36.681272 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:36.681323 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:36.681408 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.683424 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:36.692403 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.692837 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:36.695200 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:15:36.706900 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:36.706984 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:36.707047 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:36.707107 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.707192 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.707603 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.707693 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.708032 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.708699 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.711331 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.711718 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.711805 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:36.711858 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:36.711936 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.712158 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:36.712275 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:36.712324 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.715496 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.715587 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.717676 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.717760 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:36.717893 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:36.721704 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.724761 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.724855 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.725095 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.725177 140223234692160 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0122 03:15:36.725267 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:36.725306 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:36.725338 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:36.725384 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.727535 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:36.736727 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.737177 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:36.739384 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:15:36.750993 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:36.751075 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:36.751119 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:36.751170 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.751253 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.751648 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.751738 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.752190 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.752864 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.755042 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.755353 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.755432 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:36.755468 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:36.755527 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.755734 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:36.755859 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:36.755913 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.760580 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.760679 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.763301 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.763384 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:36.763482 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:36.767841 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.771021 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.771112 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.771357 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.771444 140223234692160 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0122 03:15:36.771540 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:36.771587 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:36.771621 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:36.771677 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.774433 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:36.785033 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.785474 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:36.787747 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:15:36.799274 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:36.799361 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:36.799404 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:36.799455 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.799549 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.799983 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.800086 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.800544 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.801477 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.803678 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.804020 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.804111 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:36.804152 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:36.804213 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.804474 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:36.804614 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:36.804693 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.809332 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.809430 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.811530 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.811614 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:36.811720 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:36.816776 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.820284 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.820389 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.820645 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.820746 140223234692160 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0122 03:15:36.820844 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:36.820890 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:36.820931 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:36.820998 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.823789 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:36.833693 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.834105 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:36.836301 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:15:36.847701 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:36.847777 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:36.847820 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:36.847870 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.847946 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.848366 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.848456 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.848900 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.849586 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.851866 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.852221 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.852318 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:36.852365 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:36.852433 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.852705 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:36.852853 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:36.852905 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.857349 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.857475 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.859947 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.860051 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:36.860167 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:36.863909 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:36.867039 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.867148 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:36.867413 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.867511 140223234692160 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0122 03:15:36.867619 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:36.867667 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:36.867717 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:36.867788 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.870565 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:36.879787 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.880275 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:36.882494 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:15:36.894232 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:36.894326 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:36.894385 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:36.894446 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.894562 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.895016 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.895106 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.895562 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.896496 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.899998 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.900427 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.900528 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:36.900575 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:36.900639 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:36.900921 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:36.901057 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:36.901107 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:37.009740 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:37.009932 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:37.013514 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:37.013689 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:37.013843 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:37.019778 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:37.024905 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:37.025085 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:37.025466 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:37.025589 140223234692160 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0122 03:15:37.025723 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:15:37.025781 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:15:37.025831 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:15:37.025902 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:37.029058 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:15:37.042392 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:37.043027 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:15:37.045797 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:15:37.058468 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:15:37.058575 140223234692160 attention.py:418] Single window, no scan.
I0122 03:15:37.058647 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:15:37.058709 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:37.058803 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:37.059156 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:37.059237 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:37.059793 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:37.060760 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:37.063949 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:37.064363 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:37.064463 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:15:37.064519 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:15:37.064602 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:37.064872 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:15:37.065023 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:15:37.065079 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:37.068802 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:37.068935 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:37.071299 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:37.071407 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:15:37.071536 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:15:37.075383 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:15:37.079447 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:37.079586 140223234692160 nn_components.py:261] mlp: residual
I0122 03:15:37.079888 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:37.079995 140223234692160 decoder_stack.py:344] dstack: Final layernorm.
I0122 03:15:37.082592 140223234692160 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:15:52.350284 140223234692160 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/optim/base.py:49: DeprecationWarning: Use `optax` instead of `flax.optim`. Refer to the update guide https://flax.readthedocs.io/en/latest/howtos/optax_update_guide.html for detailed instructions.
  warnings.warn(
I0122 03:15:52.661945 140223234692160 training_loop.py:409] No working directory specified.
I0122 03:15:52.662052 140223234692160 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0122 03:15:52.662446 140223234692160 checkpoints.py:425] Found no checkpoint files in ag_ckpt_vocab with prefix checkpoint_
I0122 03:15:52.662530 140223234692160 training_loop.py:447] Only restoring trainable parameters.
I0122 03:15:52.662989 140223234692160 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0122 03:15:52.663050 140223234692160 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0122 03:15:52.663100 140223234692160 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:15:52.663145 140223234692160 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:15:52.663188 140223234692160 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.663231 140223234692160 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:52.663273 140223234692160 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.663315 140223234692160 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.663357 140223234692160 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:15:52.663398 140223234692160 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:15:52.663439 140223234692160 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.663480 140223234692160 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:52.663537 140223234692160 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:15:52.663603 140223234692160 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:15:52.663667 140223234692160 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.663732 140223234692160 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:52.663796 140223234692160 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.663860 140223234692160 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.663923 140223234692160 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:15:52.663987 140223234692160 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:15:52.664051 140223234692160 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.664114 140223234692160 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:52.664178 140223234692160 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:15:52.664242 140223234692160 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:15:52.664305 140223234692160 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.664370 140223234692160 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:52.664439 140223234692160 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.664504 140223234692160 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.664567 140223234692160 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:15:52.664632 140223234692160 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:15:52.664695 140223234692160 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.664759 140223234692160 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:52.664823 140223234692160 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:15:52.664886 140223234692160 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:15:52.664951 140223234692160 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.665014 140223234692160 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:52.665078 140223234692160 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.665143 140223234692160 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.665207 140223234692160 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:15:52.665271 140223234692160 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:15:52.665335 140223234692160 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.665399 140223234692160 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:52.665462 140223234692160 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:15:52.665526 140223234692160 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:15:52.665590 140223234692160 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.665653 140223234692160 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:52.665717 140223234692160 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.665781 140223234692160 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.665844 140223234692160 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:15:52.665908 140223234692160 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:15:52.665972 140223234692160 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.666035 140223234692160 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:52.666100 140223234692160 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:15:52.666168 140223234692160 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:15:52.666232 140223234692160 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.666295 140223234692160 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:52.666359 140223234692160 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.666423 140223234692160 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.666486 140223234692160 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:15:52.666557 140223234692160 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:15:52.666622 140223234692160 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.666686 140223234692160 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:52.666749 140223234692160 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:15:52.666813 140223234692160 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:15:52.666877 140223234692160 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.666940 140223234692160 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:52.667004 140223234692160 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.667068 140223234692160 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.667131 140223234692160 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:15:52.667195 140223234692160 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:15:52.667259 140223234692160 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.667323 140223234692160 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:52.667387 140223234692160 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:15:52.667450 140223234692160 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:15:52.667514 140223234692160 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.667577 140223234692160 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:52.667641 140223234692160 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.667705 140223234692160 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.667768 140223234692160 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:15:52.667836 140223234692160 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:15:52.667901 140223234692160 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.667965 140223234692160 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:52.668028 140223234692160 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:15:52.668092 140223234692160 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:15:52.668155 140223234692160 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.668214 140223234692160 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:52.668271 140223234692160 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.668328 140223234692160 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.668385 140223234692160 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:15:52.668443 140223234692160 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:15:52.668510 140223234692160 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.668568 140223234692160 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:52.668625 140223234692160 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:15:52.668681 140223234692160 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:15:52.668738 140223234692160 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.668795 140223234692160 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:52.668852 140223234692160 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.668909 140223234692160 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.668965 140223234692160 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:15:52.669022 140223234692160 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:15:52.669079 140223234692160 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.669135 140223234692160 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:52.669192 140223234692160 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:15:52.669249 140223234692160 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:15:52.669305 140223234692160 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.669369 140223234692160 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:52.669442 140223234692160 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.669508 140223234692160 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.669566 140223234692160 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:15:52.669626 140223234692160 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:15:52.669694 140223234692160 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.669762 140223234692160 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:52.669846 140223234692160 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:15:52.669925 140223234692160 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:15:52.669995 140223234692160 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.670061 140223234692160 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:52.670145 140223234692160 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.670224 140223234692160 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.670303 140223234692160 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:15:52.670381 140223234692160 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:15:52.670460 140223234692160 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:15:52.670547 140223234692160 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:15:52.670610 140223234692160 training_loop.py:725] Total parameters: 152072288
I0122 03:15:52.670890 140223234692160 training_loop.py:739] Total state size: 0
I0122 03:15:52.781470 140223234692160 training_loop.py:492] Training loop: creating task for mode beam_search
I0122 03:15:52.781662 140223234692160 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0122 03:15:52.781921 140223234692160 training_loop.py:652] Compiling mode beam_search with jit.
I0122 03:15:52.782195 140223234692160 training_loop.py:89] registering functions: dict_keys([])
I0122 03:15:52.786290 140223234692160 graph.py:498] translated_imo_2000_p6
I0122 03:15:52.786387 140223234692160 graph.py:499] a b c = triangle a b c; d = orthocenter d a b c; e f g h = incenter2 e f g h a b c; i = foot i a b c; j = foot j b c a; k = foot k c a b; l = reflect l i e f; m = reflect m j e f; n = reflect n j f g; o = reflect o k f g; p = on_line p l m, on_line p n o ? cong h p h e
I0122 03:15:56.011133 140223234692160 ddar.py:60] Depth 1/1000 time = 3.1392853260040283
I0122 03:16:01.690372 140223234692160 ddar.py:60] Depth 2/1000 time = 5.679109334945679
I0122 03:16:17.916343 140223234692160 ddar.py:60] Depth 3/1000 time = 16.22582483291626
I0122 03:16:35.042649 140223234692160 ddar.py:60] Depth 4/1000 time = 17.1261305809021
I0122 03:16:57.219494 140223234692160 ddar.py:60] Depth 5/1000 time = 22.17665433883667
I0122 03:17:20.965695 140223234692160 ddar.py:60] Depth 6/1000 time = 23.745996475219727
I0122 03:17:45.063109 140223234692160 ddar.py:60] Depth 7/1000 time = 24.09719181060791
I0122 03:18:10.073601 140223234692160 ddar.py:60] Depth 8/1000 time = 25.010308265686035
I0122 03:18:34.539783 140223234692160 ddar.py:60] Depth 9/1000 time = 24.407445192337036
I0122 03:18:59.724370 140223234692160 ddar.py:60] Depth 10/1000 time = 25.06889772415161
I0122 03:19:24.046111 140223234692160 ddar.py:60] Depth 11/1000 time = 24.20903778076172
I0122 03:19:50.744390 140223234692160 ddar.py:60] Depth 12/1000 time = 26.67737889289856
I0122 03:19:50.744610 140223234692160 alphageometry.py:221] DD+AR failed to solve the problem.
I0122 03:19:50.744706 140223234692160 alphageometry.py:539] Depth 0. There are 1 nodes to expand:
I0122 03:19:50.744764 140223234692160 alphageometry.py:543] {S} a : ; b : ; c : ; d : T a c b d 00 T a d b c 01 ; e : C b c e 02 T b c e h 03 ; f : C a c f 04 T a c f h 05 ; g : C a b g 06 T a b g h 07 ; h : ^ a b a h a h a c 08 ^ c a c h c h c b 09 ; i : C b c i 10 T a i b c 11 ; j : C a c j 12 T a c b j 13 ; k : C a b k 14 T a b c k 15 ; l : D e i e l 16 D f i f l 17 ; m : D e j e m 18 D f j f m 19 ; n : D f j f n 20 D g j g n 21 ; o : D f k f o 22 D g k g o 23 ; p : C l m p 24 C n o p 25 ? D h p h e {F1} x00
I0122 03:19:50.744820 140223234692160 alphageometry.py:548] Decoding from {S} a : ; b : ; c : ; d : T a c b d 00 T a d b c 01 ; e : C b c e 02 T b c e h 03 ; f : C a c f 04 T a c f h 05 ; g : C a b g 06 T a b g h 07 ; h : ^ a b a h a h a c 08 ^ c a c h c h c b 09 ; i : C b c i 10 T a i b c 11 ; j : C a c j 12 T a c b j 13 ; k : C a b k 14 T a b c k 15 ; l : D e i e l 16 D f i f l 17 ; m : D e j e m 18 D f j f m 19 ; n : D f j f n 20 D g j g n 21 ; o : D f k f o 22 D g k g o 23 ; p : C l m p 24 C n o p 25 ? D h p h e {F1} x00
I0122 03:19:50.797623 140223234692160 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.797767 140223234692160 decoder_stack.py:316] dstack: scanning over 1 windows.
I0122 03:19:50.797866 140223234692160 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0122 03:19:50.797935 140223234692160 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0122 03:19:50.798005 140223234692160 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0122 03:19:50.798072 140223234692160 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0122 03:19:50.798140 140223234692160 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0122 03:19:50.798209 140223234692160 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0122 03:19:50.798279 140223234692160 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0122 03:19:50.798347 140223234692160 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0122 03:19:50.798415 140223234692160 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0122 03:19:50.798484 140223234692160 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0122 03:19:50.798557 140223234692160 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0122 03:19:50.798626 140223234692160 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0122 03:19:50.798682 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:50.798741 140223234692160 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0122 03:19:50.798868 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:50.798919 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:50.798971 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:50.800714 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.803346 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:50.812325 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.812761 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:50.814925 140223234692160 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:19:50.819580 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:50.819659 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:50.819720 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:50.819783 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.819878 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.820309 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.820404 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.820888 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.822175 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.824455 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.824846 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.824941 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:50.824997 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:50.825081 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.825298 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:50.825564 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:50.825616 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:50.828949 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.829070 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:50.832146 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.832325 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:50.832812 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:50.839009 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:50.842779 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.842919 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:50.843194 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.843295 140223234692160 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0122 03:19:50.843421 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:50.843471 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:50.843525 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:50.845352 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.847483 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:50.856681 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.857126 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:50.859396 140223234692160 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:19:50.863797 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:50.863866 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:50.863927 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:50.863989 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.864080 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.864416 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.864495 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.864835 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.865600 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.867694 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.868021 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.868102 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:50.868157 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:50.868232 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.868438 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:50.868687 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:50.868739 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:50.871987 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.872112 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:50.874304 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.874392 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:50.874711 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:50.878334 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:50.882254 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.882384 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:50.882687 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.882784 140223234692160 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0122 03:19:50.882906 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:50.882956 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:50.883010 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:50.884300 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.886402 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:50.896352 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.896819 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:50.899039 140223234692160 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:19:50.903451 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:50.903546 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:50.903613 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:50.903675 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.903769 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.904113 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.904193 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.904541 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.905328 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.907871 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.908279 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.908375 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:50.908433 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:50.908518 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.908784 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:50.909063 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:50.909116 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:50.912380 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.912481 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:50.914648 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.914735 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:50.915044 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:50.918610 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:50.921876 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.922006 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:50.922284 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.922383 140223234692160 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0122 03:19:50.922516 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:50.922574 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:50.922628 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:50.923928 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.927024 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:50.936164 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.936609 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:50.938927 140223234692160 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:19:50.944468 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:50.944565 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:50.944616 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:50.944663 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.944742 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.945062 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.945135 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.945464 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.946228 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.948382 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.948721 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.948799 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:50.948835 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:50.948889 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.949089 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:50.949348 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:50.949394 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:50.952583 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.952669 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:50.954875 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.954987 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:50.955319 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:50.959803 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:50.963063 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.963176 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:50.963449 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.963544 140223234692160 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0122 03:19:50.963664 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:50.963714 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:50.963767 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:50.965029 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.967035 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:50.977813 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.978282 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:50.980502 140223234692160 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:19:50.984844 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:50.984911 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:50.984972 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:50.985044 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.985134 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.985481 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.985561 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.986008 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.986805 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.988919 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.989270 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.989358 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:50.989414 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:50.989495 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.989709 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:50.989981 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:50.990035 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:50.994268 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.994403 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:50.996601 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:50.996689 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:50.997004 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:51.000454 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.003735 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.003831 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.004102 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.004195 140223234692160 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0122 03:19:51.004314 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:51.004362 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:51.004415 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:51.005664 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.008162 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:51.017378 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.017821 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:51.020086 140223234692160 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:19:51.024399 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:51.024481 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:51.024549 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:51.024611 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.024718 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.025056 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.025137 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.025480 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.026259 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.028428 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.028757 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.028838 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:51.028894 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:51.028970 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.029174 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:51.029424 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:51.029477 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.032953 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.033090 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.035502 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.035621 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:51.035955 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:51.039886 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.043414 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.043540 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.043798 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.043888 140223234692160 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0122 03:19:51.043989 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:51.044032 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:51.044072 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:51.045425 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.047492 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:51.056655 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.057120 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:51.059341 140223234692160 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:19:51.063800 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:51.063865 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:51.063925 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:51.063986 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.064068 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.064406 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.064485 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.064816 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.065874 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.068158 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.068478 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.068559 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:51.068613 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:51.068689 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.068897 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:51.069147 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:51.069199 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.072391 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.072495 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.074651 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.074738 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:51.075048 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:51.078478 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.081541 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.081634 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.081903 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.081996 140223234692160 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0122 03:19:51.082115 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:51.082164 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:51.082217 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:51.083474 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.085469 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:51.094488 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.094915 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:51.097112 140223234692160 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:19:51.101373 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:51.101450 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:51.101512 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:51.101574 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.101659 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.102033 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.102113 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.102453 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.103245 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.105324 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.105673 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.105767 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:51.105822 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:51.105896 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.106129 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:51.106371 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:51.106423 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.109566 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.109657 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.111800 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.111882 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:51.112186 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:51.115629 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.118880 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.118973 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.119242 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.119334 140223234692160 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0122 03:19:51.119453 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:51.119502 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:51.119556 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:51.121162 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.123166 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:51.132306 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.132763 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:51.135005 140223234692160 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:19:51.139299 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:51.139365 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:51.139428 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:51.139490 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.139578 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.139914 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.140001 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.140344 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.141107 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.143210 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.143536 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.143618 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:51.143673 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:51.143748 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.143952 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:51.144201 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:51.144253 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.147410 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.147501 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.149626 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.149711 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:51.150020 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:51.153457 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.156522 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.156617 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.156888 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.156981 140223234692160 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0122 03:19:51.157100 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:51.157149 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:51.157202 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:51.158453 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.160444 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:51.169449 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.169888 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:51.172119 140223234692160 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:19:51.176572 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:51.176650 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:51.176711 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:51.176774 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.176859 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.177200 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.177293 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.177639 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.178403 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.180504 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.180832 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.180914 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:51.180969 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:51.181042 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.181249 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:51.181493 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:51.181545 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.184680 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.184773 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.186913 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.186997 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:51.187303 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:51.190726 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.193810 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.193904 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.194173 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.194265 140223234692160 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0122 03:19:51.194385 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:51.194434 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:51.194487 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:51.195729 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.197725 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:51.206831 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.207302 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:51.209547 140223234692160 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:19:51.213854 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:51.213918 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:51.213979 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:51.214041 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.214128 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.214462 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.214546 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.214888 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.215667 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.217769 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.218096 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.218178 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:51.218233 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:51.218310 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.218551 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:51.218801 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:51.218854 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.222008 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.222099 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.224237 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.224320 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:51.224619 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:51.228032 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.231287 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.231379 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.231624 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.231712 140223234692160 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0122 03:19:51.231810 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:51.231857 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:51.231891 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:51.233565 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.236397 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:51.245436 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.245847 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:51.248068 140223234692160 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:19:51.252334 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:51.252410 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:51.252452 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:51.252504 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.252582 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.252984 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.253090 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.253536 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.254635 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.256732 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.257069 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.257165 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:51.257203 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:51.257261 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.257487 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:51.257831 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:51.257889 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.262589 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.262680 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.264811 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.264895 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:51.265177 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:51.268601 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.271657 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.271748 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.271994 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.272222 140223234692160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0122 03:19:51.272282 140223234692160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0122 03:19:51.272328 140223234692160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0122 03:19:51.272379 140223234692160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0122 03:19:51.272420 140223234692160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0122 03:19:51.272480 140223234692160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0122 03:19:51.272539 140223234692160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0122 03:19:51.272597 140223234692160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0122 03:19:51.272655 140223234692160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0122 03:19:51.272712 140223234692160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0122 03:19:51.272770 140223234692160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0122 03:19:51.272828 140223234692160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0122 03:19:51.272880 140223234692160 decoder_stack.py:344] dstack: Final layernorm.
I0122 03:19:51.276462 140223234692160 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:19:51.337155 140223234692160 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.337274 140223234692160 decoder_stack.py:333] dstack: autoregressive generator.
I0122 03:19:51.337324 140223234692160 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0122 03:19:51.337424 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:51.337473 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:51.337505 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:51.337563 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.339755 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:51.349241 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.349676 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:51.351889 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:19:51.363614 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:51.363695 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:51.363738 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:51.363775 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.363858 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.364236 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.364319 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.364770 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.365706 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.368122 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.368445 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.368521 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:51.368564 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:51.368624 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.368847 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:51.368975 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:51.369027 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.372898 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.373013 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.375318 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.375399 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:51.375505 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:51.379101 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.382279 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.382387 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.382678 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.382768 140223234692160 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0122 03:19:51.382889 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:51.382942 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:51.383002 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:51.383072 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.385885 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:51.396402 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.396876 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:51.399269 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:19:51.413340 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:51.413441 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:51.413512 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:51.413578 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.413670 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.414113 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.414205 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.414686 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.415401 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.418226 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.418768 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.418889 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:51.418956 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:51.419041 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.419337 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:51.419492 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:51.419546 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.424184 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.424337 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.426920 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.427083 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:51.427251 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:51.431950 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.435329 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.435475 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.435786 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.435899 140223234692160 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0122 03:19:51.436050 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:51.436114 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:51.436164 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:51.436248 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.439190 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:51.451329 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.451800 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:51.455137 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:19:51.470948 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:51.471042 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:51.471095 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:51.471154 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.471243 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.471558 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.471631 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.471957 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.472612 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.474935 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.475453 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.475608 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:51.475668 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:51.475753 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.476076 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:51.476239 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:51.476305 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.481058 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.481193 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.483589 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.483690 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:51.483808 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:51.487688 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.490874 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.490993 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.491268 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.491358 140223234692160 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0122 03:19:51.491470 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:51.491517 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:51.491561 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:51.491645 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.494553 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:51.507137 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.507732 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:51.510337 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:19:51.521990 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:51.522085 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:51.522143 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:51.522202 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.522321 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.522745 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.522836 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.523174 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.523835 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.526040 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.526470 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.526584 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:51.526623 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:51.526685 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.526935 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:51.527065 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:51.527148 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.531327 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.531417 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.533516 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.533598 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:51.533696 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:51.537266 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.540454 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.540580 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.540979 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.541102 140223234692160 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0122 03:19:51.541241 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:51.541303 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:51.541357 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:51.541430 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.544527 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:51.554469 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.554924 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:51.557285 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:19:51.568674 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:51.568758 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:51.568807 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:51.568861 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.568938 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.569390 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.569496 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.569980 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.570951 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.574080 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.574430 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.574530 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:51.574575 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:51.574635 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.574862 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:51.574997 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:51.575081 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.579362 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.579502 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.581761 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.581886 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:51.581997 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:51.585640 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.588916 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.589052 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.589323 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.589420 140223234692160 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0122 03:19:51.589530 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:51.589578 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:51.589614 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:51.589683 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.592633 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:51.601913 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.602346 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:51.604598 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:19:51.616263 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:51.616348 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:51.616398 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:51.616450 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.616572 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.616993 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.617094 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.617426 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.618077 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.620648 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.621154 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.621269 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:51.621330 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:51.621406 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.621695 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:51.621833 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:51.621886 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.625226 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.625316 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.627456 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.627534 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:51.627637 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:51.631028 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.634063 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.634149 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.634404 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.634487 140223234692160 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0122 03:19:51.634592 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:51.634639 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:51.634672 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:51.634718 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.637497 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:51.646691 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.647155 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:51.649375 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:19:51.660845 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:51.660929 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:51.660969 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:51.661006 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.661099 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.661434 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.661507 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.661938 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.662785 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.664945 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.665282 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.665370 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:51.665405 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:51.665454 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.665653 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:51.665749 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:51.665798 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.670362 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.670473 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.672577 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.672669 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:51.672770 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:51.676390 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.679424 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.679522 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.679787 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.679883 140223234692160 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0122 03:19:51.679986 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:51.680053 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:51.680086 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:51.680133 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.682107 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:51.690997 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.691437 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:51.693669 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:19:51.705097 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:51.705175 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:51.705214 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:51.705260 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.705366 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.705755 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.705842 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.706292 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.707029 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.709174 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.709508 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.709596 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:51.709632 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:51.709678 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.709905 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:51.710002 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:51.710052 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.714604 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.714699 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.716779 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.716869 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:51.716969 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:51.720541 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.723673 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.723795 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.724061 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.724159 140223234692160 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0122 03:19:51.724267 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:51.724335 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:51.724368 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:51.724415 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.726989 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:51.736189 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.736685 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:51.738963 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:19:51.750390 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:51.750466 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:51.750506 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:51.750560 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.750653 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.750980 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.751053 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.751477 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.752165 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.754305 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.754667 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.754756 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:51.754792 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:51.754840 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.755066 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:51.755164 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:51.755212 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.759292 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.759419 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.761534 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.761628 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:51.761731 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:51.765797 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.768932 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.769029 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.769286 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.769372 140223234692160 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0122 03:19:51.769474 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:51.769526 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:51.769571 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:51.769635 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.771647 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:51.781457 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.781911 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:51.784178 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:19:51.796935 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:51.797033 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:51.797091 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:51.797156 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.797260 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.797698 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.797790 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.798262 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.799241 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.802405 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.802818 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.802910 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:51.802947 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:51.803013 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.803267 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:51.803390 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:51.803443 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.806682 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.806795 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.808951 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.809033 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:51.809132 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:51.812575 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.815793 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.815902 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.816172 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.816268 140223234692160 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0122 03:19:51.816391 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:51.816447 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:51.816498 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:51.816571 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.819385 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:51.829089 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.829709 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:51.832592 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:19:51.844186 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:51.844283 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:51.844338 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:51.844389 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.844483 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.844929 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.845022 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.845494 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.846193 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.848428 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.848743 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.848819 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:51.848854 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:51.848903 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.849094 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:51.849191 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:51.849238 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.852358 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.852462 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.854821 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.854917 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:51.855032 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:51.860518 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.863737 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.863848 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.864114 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.864210 140223234692160 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0122 03:19:51.864336 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:51.864391 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:51.864443 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:51.864515 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.867333 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:51.877025 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.877632 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:51.880884 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:19:51.892538 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:51.892628 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:51.892699 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:51.892767 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.892874 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.893307 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.893397 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.893751 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.894417 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.896617 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.896933 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.897009 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:51.897045 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:51.897095 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.897285 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:51.897383 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:51.897431 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.900561 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.900670 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.902947 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.903041 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:51.903143 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:51.907635 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:51.910993 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.911119 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:51.911392 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.911494 140223234692160 decoder_stack.py:344] dstack: Final layernorm.
I0122 03:19:51.914092 140223234692160 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:19:51.983572 140223234692160 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:51.983720 140223234692160 decoder_stack.py:333] dstack: autoregressive generator.
I0122 03:19:51.983797 140223234692160 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0122 03:19:51.983915 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:51.983963 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:51.984015 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:51.984094 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:51.987051 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:51.995941 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:51.996394 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:51.998628 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:19:52.009941 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:52.010023 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:52.010082 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:52.010128 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.010200 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.010509 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.010598 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.011040 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.011713 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.013820 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.014150 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.014237 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:52.014273 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:52.014319 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.014571 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:52.014688 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:52.014730 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:52.017765 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.017850 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:52.019978 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.020104 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:52.020232 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:52.026113 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:52.030245 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.030376 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:52.030672 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.030760 140223234692160 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0122 03:19:52.030852 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:52.030891 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:52.030923 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:52.030971 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.032996 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:52.043775 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.044289 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:52.046627 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:19:52.064277 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:52.064411 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:52.064475 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:52.064532 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.064641 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.065129 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.065237 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.065739 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.066791 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.070272 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.070843 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.070984 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:52.071056 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:52.071150 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.071471 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:52.071633 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:52.071701 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:52.076937 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.077129 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:52.080384 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.080555 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:52.080712 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:52.086238 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:52.091257 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.091446 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:52.091897 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.092033 140223234692160 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0122 03:19:52.092191 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:52.092256 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:52.092320 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:52.092409 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.095669 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:52.110346 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.111017 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:52.113964 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:19:52.125548 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:52.125632 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:52.125677 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:52.125715 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.125789 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.126093 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.126166 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.126481 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.127156 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.129268 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.129570 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.129645 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:52.129680 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:52.129728 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.129900 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:52.129995 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:52.130035 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:52.133208 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.133307 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:52.135447 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.135527 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:52.135617 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:52.139044 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:52.142098 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.142191 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:52.142432 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.142517 140223234692160 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0122 03:19:52.142614 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:52.142653 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:52.142686 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:52.142732 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.144681 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:52.153675 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.154068 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:52.156409 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:19:52.167738 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:52.167811 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:52.167851 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:52.167888 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.167982 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.168282 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.168355 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.168664 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.169304 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.171514 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.171842 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.171930 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:52.171966 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:52.172015 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.172214 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:52.172308 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:52.172348 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:52.175414 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.175497 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:52.177554 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.177643 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:52.177733 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:52.181493 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:52.184542 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.184642 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:52.184890 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.184986 140223234692160 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0122 03:19:52.185077 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:52.185117 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:52.185149 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:52.185194 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.187142 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:52.196094 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.196506 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:52.198771 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:19:52.210201 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:52.210291 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:52.210332 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:52.210378 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.210458 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.210779 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.210852 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.211174 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.211832 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.213948 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.214277 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.214364 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:52.214400 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:52.214462 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.214663 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:52.214762 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:52.214805 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:52.217862 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.217945 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:52.220021 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.220098 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:52.220194 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:52.223541 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:52.226525 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.226610 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:52.226857 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.226939 140223234692160 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0122 03:19:52.227036 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:52.227077 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:52.227109 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:52.227154 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.229097 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:52.238170 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.238627 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:52.240789 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:19:52.251998 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:52.252071 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:52.252120 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:52.252168 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.252236 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.252548 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.252621 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.252946 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.253619 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.255851 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.256194 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.256288 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:52.256333 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:52.256392 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.256596 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:52.256712 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:52.256753 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:52.259839 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.259922 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:52.261997 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.262073 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:52.262170 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:52.265582 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:52.268606 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.268690 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:52.268935 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.269019 140223234692160 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0122 03:19:52.269115 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:52.269157 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:52.269189 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:52.269234 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.271211 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:52.280431 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.280879 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:52.283112 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:19:52.295087 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:52.295172 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:52.295214 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:52.295261 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.295340 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.295651 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.295723 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.296042 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.296691 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.298796 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.299113 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.299186 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:52.299221 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:52.299268 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.299464 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:52.299559 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:52.299604 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:52.302681 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.302763 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:52.304842 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.304917 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:52.305006 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:52.309324 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:52.312401 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.312503 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:52.312746 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.312830 140223234692160 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0122 03:19:52.312919 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:52.312959 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:52.312990 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:52.313035 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.314984 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:52.324399 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.324842 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:52.327068 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:19:52.338446 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:52.338550 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:52.338610 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:52.338665 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.338783 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.339276 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.339365 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.339821 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.340557 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.342772 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.343129 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.343205 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:52.343252 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:52.343334 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.343567 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:52.343700 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:52.343755 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:52.348445 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.348546 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:52.350668 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.350747 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:52.350855 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:52.354282 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:52.357706 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.357838 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:52.358127 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.358215 140223234692160 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0122 03:19:52.358328 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:52.358376 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:52.358430 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:52.358507 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.360556 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:52.369715 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.370143 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:52.372829 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:19:52.384408 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:52.384491 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:52.384554 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:52.384611 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.384705 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.385041 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.385118 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.385459 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.386115 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.388231 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.388562 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.388657 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:52.388710 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:52.388782 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.389034 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:52.389182 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:52.389244 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:52.394408 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.394550 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:52.396686 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.396770 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:52.396878 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:52.403798 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:52.408080 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.408209 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:52.408464 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.408550 140223234692160 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0122 03:19:52.408641 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:52.408681 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:52.408714 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:52.408785 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.412572 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:52.426470 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.426905 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:52.430492 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:19:52.453012 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:52.453130 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:52.453199 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:52.453273 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.453390 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.453932 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.454044 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.454636 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.455860 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.459513 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.459936 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.460026 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:52.460072 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:52.460136 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.460379 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:52.460500 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:52.460550 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:52.465186 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.465283 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:52.467432 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.467509 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:52.467598 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:52.471263 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:52.474318 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.474410 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:52.474656 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.474740 140223234692160 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0122 03:19:52.474829 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:52.474868 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:52.474900 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:52.474944 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.476890 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:52.485761 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.486158 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:52.488336 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:19:52.499592 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:52.499668 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:52.499708 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:52.499746 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.499839 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.500131 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.500211 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.500522 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.501163 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.503265 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.503587 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.503672 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:52.503708 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:52.503754 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.503922 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:52.504041 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:52.504081 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:52.507100 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.507182 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:52.509268 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.509344 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:52.509432 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:52.512772 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:52.515764 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.515848 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:52.516087 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.516168 140223234692160 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0122 03:19:52.516257 140223234692160 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:19:52.516296 140223234692160 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:19:52.516328 140223234692160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:19:52.516372 140223234692160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.518301 140223234692160 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:19:52.527302 140223234692160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.527697 140223234692160 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:19:52.529878 140223234692160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:19:52.541489 140223234692160 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:19:52.541579 140223234692160 attention.py:418] Single window, no scan.
I0122 03:19:52.541637 140223234692160 transformer_layer.py:389] tlayer: self-attention.
I0122 03:19:52.541702 140223234692160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.541800 140223234692160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.542179 140223234692160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.542272 140223234692160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.542658 140223234692160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.543399 140223234692160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.545610 140223234692160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.545973 140223234692160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.546064 140223234692160 transformer_layer.py:468] tlayer: End windows.
I0122 03:19:52.546117 140223234692160 transformer_layer.py:472] tlayer: final FFN.
I0122 03:19:52.546193 140223234692160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.546399 140223234692160 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:19:52.546533 140223234692160 nn_components.py:325] mlp: activation = None
I0122 03:19:52.546582 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:52.549741 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.549858 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:52.552036 140223234692160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.552127 140223234692160 transformer_base.py:443] tbase: final FFN
I0122 03:19:52.552245 140223234692160 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:19:52.556053 140223234692160 nn_components.py:329] mlp: final activation = None
I0122 03:19:52.561034 140223234692160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.561191 140223234692160 nn_components.py:261] mlp: residual
I0122 03:19:52.561509 140223234692160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:19:52.561658 140223234692160 decoder_stack.py:344] dstack: Final layernorm.
I0122 03:19:52.565407 140223234692160 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:17.266232 140223234692160 alphageometry.py:565] LM output (score=-22.654367): "99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99"
I0122 03:22:17.266352 140223234692160 alphageometry.py:566] Translation: "ERROR: must end with ;"

I0122 03:22:17.266400 140223234692160 alphageometry.py:565] LM output (score=-27.337936): "99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99****************************************************************************************************************************************************************************************************************************"
I0122 03:22:17.266435 140223234692160 alphageometry.py:566] Translation: "ERROR: must end with ;"

I0122 03:22:17.266469 140223234692160 alphageometry.py:539] Depth 1. There are 0 nodes to expand:
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
2024-01-22 03:22:19.522499: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
I0122 03:22:21.037571 140480029578304 inference_utils.py:69] Parsing gin configuration.
I0122 03:22:21.037664 140480029578304 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0122 03:22:21.037858 140480029578304 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0122 03:22:21.037895 140480029578304 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0122 03:22:21.037933 140480029578304 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0122 03:22:21.037970 140480029578304 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0122 03:22:21.038038 140480029578304 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0122 03:22:21.038068 140480029578304 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0122 03:22:21.038096 140480029578304 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0122 03:22:21.038124 140480029578304 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=2
I0122 03:22:21.038152 140480029578304 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0122 03:22:21.038180 140480029578304 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0122 03:22:21.038230 140480029578304 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0122 03:22:21.038357 140480029578304 resource_reader.py:55] Path not found: base_htrans.gin
I0122 03:22:21.038527 140480029578304 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0122 03:22:21.038644 140480029578304 resource_reader.py:55] Path not found: trainer_configuration.gin
I0122 03:22:21.043979 140480029578304 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0122 03:22:21.044126 140480029578304 resource_reader.py:55] Path not found: size/medium_150M.gin
I0122 03:22:21.044466 140480029578304 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0122 03:22:21.044593 140480029578304 resource_reader.py:55] Path not found: options/positions_t5.gin
I0122 03:22:21.044888 140480029578304 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0122 03:22:21.045013 140480029578304 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0122 03:22:21.045494 140480029578304 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0122 03:22:21.045620 140480029578304 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0122 03:22:21.049271 140480029578304 training_loop.py:334] ==== Training loop: initializing model ====
I0122 03:22:21.051494 140480029578304 xla_bridge.py:166] Remote TPU is not linked into jax; skipping remote TPU.
I0122 03:22:21.051554 140480029578304 xla_bridge.py:413] Unable to initialize backend 'tpu_driver': Could not initialize backend 'tpu_driver'
I0122 03:22:21.051613 140480029578304 xla_bridge.py:413] Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
I0122 03:22:21.051666 140480029578304 xla_bridge.py:413] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
I0122 03:22:21.052042 140480029578304 xla_bridge.py:413] Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.
I0122 03:22:21.052120 140480029578304 xla_bridge.py:413] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
W0122 03:22:21.052181 140480029578304 xla_bridge.py:420] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0122 03:22:21.052235 140480029578304 training_loop.py:335] Process 0 of 1
I0122 03:22:21.052275 140480029578304 training_loop.py:336] Local device count = 1
I0122 03:22:21.052316 140480029578304 training_loop.py:337] Number of replicas = 1
I0122 03:22:21.052348 140480029578304 training_loop.py:339] Using random number seed 42
I0122 03:22:21.230851 140480029578304 training_loop.py:359] Initializing the model.
I0122 03:22:21.318122 140480029578304 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.318351 140480029578304 decoder_stack.py:316] dstack: scanning over 1 windows.
I0122 03:22:21.318421 140480029578304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:22:21.318473 140480029578304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:22:21.318536 140480029578304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:22:21.318602 140480029578304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:22:21.318677 140480029578304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:22:21.318745 140480029578304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:22:21.318813 140480029578304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:22:21.318880 140480029578304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:22:21.318946 140480029578304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:22:21.319014 140480029578304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:22:21.319090 140480029578304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:22:21.319164 140480029578304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:22:21.319220 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:21.319283 140480029578304 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0122 03:22:21.319414 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:21.319468 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:21.319521 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:21.321575 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.327253 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:21.341519 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.341983 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:21.346041 140480029578304 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:22:21.359457 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:21.359560 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:21.359629 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:21.359692 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.359784 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.360776 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.360860 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.361934 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.364635 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.371221 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.372247 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.372346 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:21.372403 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:21.372480 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.372677 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:21.372966 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:21.373018 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:21.376261 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.376379 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:21.379198 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.379287 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:21.379628 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:21.391665 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:21.402210 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.402325 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:21.402635 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.402722 140480029578304 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0122 03:22:21.402822 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:21.402870 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:21.402906 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:21.404347 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.406499 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:21.414921 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.415358 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:21.417603 140480029578304 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:22:21.422158 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:21.422233 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:21.422275 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:21.422314 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.422391 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.422712 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.422785 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.423105 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.423868 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.426053 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.426370 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.426459 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:21.426496 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:21.426553 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.426773 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:21.427017 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:21.427065 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:21.429944 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.430037 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:21.432311 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.432402 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:21.432708 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:21.435950 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:21.438820 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.438910 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:21.439163 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.439246 140480029578304 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0122 03:22:21.439343 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:21.439388 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:21.439422 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:21.440771 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.442979 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:21.451456 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.451855 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:21.454126 140480029578304 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:22:21.458314 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:21.458387 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:21.458427 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:21.458475 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.458565 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.459005 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.459094 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.459421 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.460187 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.462315 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.462669 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.462757 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:21.462794 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:21.462853 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.463034 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:21.463274 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:21.463322 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:21.466189 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.466284 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:21.468567 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.468644 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:21.468941 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:21.472179 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:21.475381 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.475476 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:21.475729 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.475811 140480029578304 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0122 03:22:21.475910 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:21.475953 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:21.475986 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:21.563628 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.565973 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:21.574681 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.575130 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:21.577455 140480029578304 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:22:21.581637 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:21.581710 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:21.581757 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:21.581805 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.581873 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.582188 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.582261 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.582628 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.583428 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.585543 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.585882 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.585970 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:21.586007 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:21.586062 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.586253 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:21.586573 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:21.586633 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:21.589465 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.589553 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:21.591850 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.591984 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:21.592297 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:21.595551 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:21.598361 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.598446 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:21.598705 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.598790 140480029578304 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0122 03:22:21.598889 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:21.598932 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:21.598966 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:21.600347 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.602679 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:21.612468 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.613038 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:21.615872 140480029578304 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:22:21.620548 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:21.620651 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:21.620717 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:21.620781 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.620875 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.621335 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.621430 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.621800 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.622626 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.624784 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.625122 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.625203 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:21.625260 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:21.625343 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.625595 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:21.625866 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:21.625921 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:21.629028 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.629121 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:21.631453 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.631549 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:21.631872 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:21.635126 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:21.638074 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.638169 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:21.638444 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.638544 140480029578304 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0122 03:22:21.638665 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:21.638715 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:21.638771 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:21.640157 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.642295 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:21.650858 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.651279 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:21.653625 140480029578304 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:22:21.657998 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:21.658087 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:21.658152 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:21.658215 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.658315 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.658708 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.658801 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.659145 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.659930 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.662075 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.662438 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.662540 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:21.662598 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:21.662675 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.662876 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:21.663124 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:21.663179 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:21.666363 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.666457 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:21.668811 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.668896 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:21.669218 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:21.672480 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:21.675552 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.675699 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:21.676002 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.676107 140480029578304 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0122 03:22:21.676235 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:21.676287 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:21.676343 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:21.677789 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.680068 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:21.688609 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.689038 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:21.691351 140480029578304 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:22:21.695521 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:21.695587 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:21.695651 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:21.695716 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.695802 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.696139 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.696219 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.696557 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.697375 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.699529 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.699854 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.699936 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:21.699995 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:21.700073 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.700278 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:21.700534 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:21.700588 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:21.703501 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.703598 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:21.705965 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.706051 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:21.706367 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:21.709620 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:21.712523 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.712621 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:21.712895 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.712991 140480029578304 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0122 03:22:21.713111 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:21.713162 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:21.713217 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:21.714640 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.718275 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:21.726793 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.727231 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:21.729527 140480029578304 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:22:21.733706 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:21.733772 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:21.733835 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:21.733899 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.733986 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.734409 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.734500 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.734847 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.735625 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.737775 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.738108 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.738190 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:21.738249 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:21.738327 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.738572 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:21.738837 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:21.738892 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:21.741780 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.741872 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:21.744208 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.744293 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:21.744616 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:21.748048 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:21.750889 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.750990 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:21.751265 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.751359 140480029578304 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0122 03:22:21.751480 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:21.751531 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:21.751586 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:21.753005 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.755171 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:21.763556 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.764043 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:21.766256 140480029578304 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:22:21.770808 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:21.770889 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:21.770952 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:21.771015 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.771131 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.771470 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.771551 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.771892 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.772659 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.774846 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.775182 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.775264 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:21.775321 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:21.775399 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.775608 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:21.775867 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:21.775922 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:21.778839 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.778932 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:21.781255 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.781340 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:21.781659 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:21.784948 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:21.787816 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.787913 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:21.788234 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.788330 140480029578304 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0122 03:22:21.788449 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:21.788499 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:21.788554 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:21.789924 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.792060 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:21.800533 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.800962 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:21.803248 140480029578304 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:22:21.807758 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:21.807865 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:21.807929 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:21.808021 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.808137 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.808472 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.808552 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.808896 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.809663 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.811876 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.812225 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.812319 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:21.812377 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:21.812455 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.812668 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:21.813020 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:21.813083 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:21.816415 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.816523 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:21.818828 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.818916 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:21.819244 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:21.822568 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:21.825831 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.825937 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:21.826214 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.826317 140480029578304 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0122 03:22:21.826452 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:21.826508 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:21.826571 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:21.827962 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.830156 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:21.838623 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.839051 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:21.841296 140480029578304 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:22:21.845595 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:21.845661 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:21.845723 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:21.845788 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.845874 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.846211 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.846290 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.846676 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.847448 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.849630 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.849960 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.850041 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:21.850098 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:21.850176 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.850380 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:21.850672 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:21.850727 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:21.853630 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.853732 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:21.856038 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.856126 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:21.856451 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:21.859736 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:21.862648 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.862744 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:21.863016 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.863111 140480029578304 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0122 03:22:21.863240 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:21.863291 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:21.863347 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:21.864732 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.866929 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:21.875370 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.875839 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:21.878585 140480029578304 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:22:21.882851 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:21.882924 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:21.882987 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:21.883051 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.883143 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.883523 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.883604 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.883946 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.884727 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.886900 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.887227 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.887308 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:21.887367 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:21.887446 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.887648 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:21.887903 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:21.887958 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:21.890870 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.890965 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:21.893234 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.893319 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:21.893640 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:21.896918 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:21.899832 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.899931 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:21.900206 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.900461 140480029578304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:22:21.900529 140480029578304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:22:21.900609 140480029578304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:22:21.900675 140480029578304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:22:21.900738 140480029578304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:22:21.900800 140480029578304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:22:21.900861 140480029578304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:22:21.900925 140480029578304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:22:21.900988 140480029578304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:22:21.901051 140480029578304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:22:21.901113 140480029578304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:22:21.901175 140480029578304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:22:21.901232 140480029578304 decoder_stack.py:344] dstack: Final layernorm.
I0122 03:22:21.905714 140480029578304 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:21.967010 140480029578304 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:21.967146 140480029578304 decoder_stack.py:333] dstack: autoregressive generator.
I0122 03:22:21.967220 140480029578304 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0122 03:22:21.967338 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:21.967414 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:21.967469 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:21.967548 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:21.970479 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:21.980774 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:21.981223 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:21.983470 140480029578304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:21.999230 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:21.999320 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:21.999383 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:21.999447 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:21.999537 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.000443 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.000526 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.001256 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.003286 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.007797 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.008694 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.008789 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:22.008848 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:22.008929 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.009131 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:22.009247 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:22.009298 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.012450 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.012543 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.014652 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.014740 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:22.014862 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:22.018708 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.021736 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.021836 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.022109 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.022204 140480029578304 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0122 03:22:22.022323 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:22.022374 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:22.022429 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:22.022507 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.025272 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:22.035343 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.035819 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:22.038098 140480029578304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:22.050559 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:22.050666 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:22.050733 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:22.050797 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.050891 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.051246 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.051327 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.051792 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.052759 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.055129 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.055508 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.055596 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:22.055662 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:22.055746 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.055960 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:22.056079 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:22.056130 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.060020 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.060117 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.062229 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.062315 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:22.062448 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:22.065858 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.068919 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.069014 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.069285 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.069378 140480029578304 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0122 03:22:22.069497 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:22.069548 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:22.069603 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:22.069681 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.071905 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:22.081707 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.082174 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:22.084460 140480029578304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:22.096109 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:22.096195 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:22.096261 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:22.096325 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.096418 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.096755 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.096835 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.097179 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.097861 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.100070 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.100400 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.100481 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:22.100539 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:22.100632 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.100835 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:22.100952 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:22.101003 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.104168 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.104261 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.106386 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.106470 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:22.106597 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:22.109985 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.113025 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.113121 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.113390 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.113483 140480029578304 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0122 03:22:22.113601 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:22.113651 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:22.113705 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:22.113782 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.116536 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:22.125525 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.125953 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:22.128177 140480029578304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:22.139952 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:22.140026 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:22.140092 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:22.140157 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.140244 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.140583 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.140663 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.141010 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.141692 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.143882 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.144207 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.144290 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:22.144347 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:22.144427 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.144643 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:22.144759 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:22.144810 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.147945 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.148040 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.150137 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.150221 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:22.150342 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:22.153753 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.156826 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.156922 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.157195 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.157290 140480029578304 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0122 03:22:22.157410 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:22.157460 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:22.157515 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:22.157592 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.159581 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:22.168500 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.168920 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:22.171137 140480029578304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:22.182775 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:22.182862 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:22.182926 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:22.182989 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.183075 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.183417 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.183509 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.183850 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.184523 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.186713 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.187068 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.187165 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:22.187222 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:22.187300 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.187516 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:22.187644 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:22.187700 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.191133 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.191228 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.193353 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.193440 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:22.193561 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:22.197160 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.200200 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.200295 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.200563 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.200658 140480029578304 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0122 03:22:22.200778 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:22.200828 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:22.200883 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:22.200959 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.203389 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:22.212276 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.212700 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:22.214937 140480029578304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:22.226428 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:22.226512 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:22.226581 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:22.226645 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.226761 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.227106 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.227201 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.227545 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.228224 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.230385 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.230741 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.230836 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:22.230893 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:22.230970 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.231222 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:22.231376 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:22.231460 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.234597 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.234690 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.236784 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.236868 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:22.236988 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:22.240376 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.243420 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.243514 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.243785 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.243878 140480029578304 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0122 03:22:22.243998 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:22.244049 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:22.244104 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:22.244180 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.246882 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:22.256050 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.256529 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:22.258736 140480029578304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:22.270215 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:22.270284 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:22.270349 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:22.270414 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.270511 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.270907 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.270988 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.271330 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.272139 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.274350 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.274719 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.274807 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:22.274866 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:22.274948 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.275215 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:22.275359 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:22.275427 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.278752 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.278881 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.281022 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.281111 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:22.281244 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:22.284851 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.287915 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.288020 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.288295 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.288391 140480029578304 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0122 03:22:22.288511 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:22.288562 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:22.288618 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:22.288694 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.291462 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:22.300438 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.300876 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:22.303093 140480029578304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:22.314910 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:22.314984 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:22.315048 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:22.315112 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.315201 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.315543 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.315623 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.315968 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.316891 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.319566 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.319907 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.319990 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:22.320048 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:22.320126 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.320336 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:22.320452 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:22.320503 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.323678 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.323774 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.326688 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.326773 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:22.326896 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:22.330289 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.333323 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.333419 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.333692 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.333785 140480029578304 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0122 03:22:22.333906 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:22.333956 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:22.334011 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:22.334087 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.336843 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:22.345865 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.346341 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:22.348619 140480029578304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:22.360263 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:22.360340 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:22.360405 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:22.360470 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.360560 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.360901 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.360981 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.361336 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.362020 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.364221 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.364553 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.364635 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:22.364694 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:22.364774 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.364982 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:22.365097 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:22.365149 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.368312 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.368415 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.370536 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.370622 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:22.370747 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:22.374333 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.377402 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.377500 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.377775 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.377869 140480029578304 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0122 03:22:22.377991 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:22.378042 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:22.378099 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:22.378176 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.380934 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:22.389857 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.390279 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:22.392546 140480029578304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:22.404117 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:22.404199 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:22.404263 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:22.404327 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.404441 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.404776 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.404857 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.405203 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.405892 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.408120 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.408469 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.408567 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:22.408625 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:22.408703 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.408940 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:22.409056 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:22.409107 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.412225 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.412334 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.414444 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.414533 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:22.414658 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:22.418032 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.421105 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.421215 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.421537 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.421636 140480029578304 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0122 03:22:22.421774 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:22.421832 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:22.421887 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:22.421965 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.424224 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:22.433391 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.433838 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:22.436073 140480029578304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:22.447677 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:22.447764 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:22.447829 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:22.447893 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.447982 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.448323 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.448404 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.448750 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.449671 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.452074 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.452425 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.452524 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:22.452582 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:22.452661 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.452867 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:22.452988 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:22.453040 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.456178 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.456271 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.458393 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.458483 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:22.458580 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:22.461961 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.464977 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.465065 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.465305 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.465388 140480029578304 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0122 03:22:22.465479 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:22.465521 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:22.465555 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:22.465603 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.467548 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:22.476549 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.477034 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:22.479371 140480029578304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:22.491190 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:22.491266 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:22.491312 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:22.491364 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.491453 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.491870 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.491963 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.492415 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.493335 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.495556 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.495893 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.495978 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:22.496028 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:22.496099 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.496356 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:22.496486 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:22.496541 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.499717 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.499810 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.501896 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.501991 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:22.502090 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:22.505475 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.508509 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.508605 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.508864 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.508958 140480029578304 decoder_stack.py:344] dstack: Final layernorm.
I0122 03:22:22.511419 140480029578304 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:22.596364 140480029578304 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.596552 140480029578304 decoder_stack.py:333] dstack: autoregressive generator.
I0122 03:22:22.596625 140480029578304 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0122 03:22:22.596760 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:22.596823 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:22.596876 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:22.596954 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.600748 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:22.611946 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.612409 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:22.614607 140480029578304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:22.626007 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:22.626084 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:22.626133 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:22.626182 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.626258 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.626614 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.626688 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.627009 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.627655 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.629774 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.630107 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.630197 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:22.630240 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:22.630296 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.630485 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:22.630599 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:22.630647 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.633703 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.633788 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.635866 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.635958 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:22.636062 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:22.640146 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.645119 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.645278 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.645621 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.645735 140480029578304 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0122 03:22:22.645855 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:22.645910 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:22.645957 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:22.646023 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.648946 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:22.663668 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.664141 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:22.666403 140480029578304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:22.677976 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:22.678066 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:22.678111 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:22.678151 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.678234 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.678592 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.678667 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.678996 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.679658 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.681820 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.682127 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.682203 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:22.682241 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:22.682303 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.682500 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:22.682609 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:22.682658 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.685725 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.685825 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.688244 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.688324 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:22.688423 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:22.691817 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.694879 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.694980 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.695243 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.695329 140480029578304 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0122 03:22:22.695426 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:22.695470 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:22.695504 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:22.695551 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.697520 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:22.707461 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.707928 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:22.710239 140480029578304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:22.721936 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:22.722043 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:22.722116 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:22.722188 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.722305 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.722801 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.722904 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.723444 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.724315 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.726864 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.727238 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.727324 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:22.727363 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:22.727416 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.727619 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:22.727727 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:22.727775 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.731003 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.731139 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.733424 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.733540 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:22.733645 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:22.737333 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.740578 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.740707 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.740973 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.741061 140480029578304 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0122 03:22:22.741160 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:22.741204 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:22.741239 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:22.741287 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.743302 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:22.752237 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.752654 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:22.754880 140480029578304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:22.766272 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:22.766338 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:22.766381 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:22.766419 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.766495 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.766813 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.766887 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.767201 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.767848 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.769990 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.770295 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.770369 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:22.770406 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:22.770454 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.770649 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:22.770746 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:22.770796 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.773843 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.773928 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.776038 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.776115 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:22.776209 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:22.779566 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.782785 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.782871 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.783122 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.783202 140480029578304 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0122 03:22:22.783299 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:22.783341 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:22.783374 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:22.783421 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.785374 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:22.794297 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.794712 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:22.796911 140480029578304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:22.809192 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:22.809293 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:22.809364 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:22.809431 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.809532 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.809976 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.810068 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.810435 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.811152 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.813788 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.814190 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.814288 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:22.814343 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:22.814429 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.814670 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:22.814810 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:22.814865 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.819259 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.819381 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.821519 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.821614 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:22.821730 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:22.825305 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.828892 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.829031 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.829322 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.829413 140480029578304 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0122 03:22:22.829512 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:22.829554 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:22.829592 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:22.829646 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.832624 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:22.842225 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.842741 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:22.844974 140480029578304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:22.857147 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:22.857289 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:22.857338 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:22.857379 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.857452 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.857784 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.857867 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.858222 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.858930 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.861172 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.861496 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.861572 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:22.861608 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:22.861660 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.861856 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:22.861955 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:22.862002 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.866643 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.866753 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.868880 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.868970 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:22.869062 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:22.872694 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.875891 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.875985 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.876235 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.876319 140480029578304 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0122 03:22:22.876414 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:22.876461 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:22.876494 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:22.876541 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.878503 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:22.887425 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.887826 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:22.890040 140480029578304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:22.901541 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:22.901606 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:22.901646 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:22.901684 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.901760 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.902064 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.902136 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.902449 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.903118 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.905496 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.905817 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.905892 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:22.905927 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:22.905975 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.906173 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:22.906296 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:22.906349 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.909490 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.909579 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.911668 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.911746 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:22.911852 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:22.915240 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.918255 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.918341 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.918594 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.918677 140480029578304 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0122 03:22:22.918767 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:22.918812 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:22.918847 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:22.918905 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.921627 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:22.930796 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.931212 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:22.933426 140480029578304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:22.944847 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:22.944921 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:22.944962 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:22.945001 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.945104 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.945410 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.945482 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.945797 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.946455 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.948665 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.948996 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.949083 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:22.949120 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:22.949167 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.949350 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:22.949446 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:22.949492 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.952539 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.952636 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.954722 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.954813 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:22.954903 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:22.958983 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.962232 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.962344 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.962629 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.962725 140480029578304 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0122 03:22:22.962815 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:22.962862 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:22.962896 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:22.962944 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.964907 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:22.973842 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.974287 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:22.976518 140480029578304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:22.988000 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:22.988075 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:22.988114 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:22.988152 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.988226 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.988530 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.988602 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.988915 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.989579 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.991767 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.992106 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.992193 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:22.992230 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:22.992278 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.992493 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:22.992589 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:22.992636 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:22.995729 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.995814 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:22.997899 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:22.997975 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:22.998067 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:23.001437 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:23.004433 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.004534 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:23.004791 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.004888 140480029578304 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0122 03:22:23.004985 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:23.005028 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:23.005060 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:23.005106 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.007065 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:23.016111 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.016539 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:23.018778 140480029578304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:23.030290 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:23.030375 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:23.030425 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:23.030486 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.030600 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.030981 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.031068 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.031417 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.032092 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.034486 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.034851 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.034931 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:23.034968 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:23.035024 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.035219 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:23.035325 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:23.035367 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:23.038492 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.038587 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:23.041530 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.041623 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:23.041731 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:23.045123 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:23.048154 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.048251 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:23.048510 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.048595 140480029578304 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0122 03:22:23.048691 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:23.048734 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:23.048766 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:23.048811 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.050777 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:23.059704 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.060114 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:23.062316 140480029578304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:23.073767 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:23.073842 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:23.073882 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:23.073927 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.074002 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.074313 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.074386 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.074717 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.075381 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.077750 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.078074 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.078159 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:23.078196 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:23.078254 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.078457 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:23.078573 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:23.078616 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:23.183731 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.183878 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:23.186077 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.186171 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:23.186268 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:23.189880 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:23.193003 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.193109 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:23.193388 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.193488 140480029578304 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0122 03:22:23.193590 140480029578304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:23.193633 140480029578304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:23.193667 140480029578304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:23.193714 140480029578304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.195711 140480029578304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:23.204875 140480029578304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.205287 140480029578304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:23.207526 140480029578304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:23.219177 140480029578304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:23.219259 140480029578304 attention.py:418] Single window, no scan.
I0122 03:22:23.219300 140480029578304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:23.219347 140480029578304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.219424 140480029578304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.219740 140480029578304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.219812 140480029578304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.220183 140480029578304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.220842 140480029578304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.222996 140480029578304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.223306 140480029578304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.223380 140480029578304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:23.223416 140480029578304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:23.223464 140480029578304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.223650 140480029578304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:23.223756 140480029578304 nn_components.py:325] mlp: activation = None
I0122 03:22:23.223799 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:23.226951 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.227038 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:23.229130 140480029578304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.229207 140480029578304 transformer_base.py:443] tbase: final FFN
I0122 03:22:23.229303 140480029578304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:23.232747 140480029578304 nn_components.py:329] mlp: final activation = None
I0122 03:22:23.235821 140480029578304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.235907 140480029578304 nn_components.py:261] mlp: residual
I0122 03:22:23.236157 140480029578304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:23.236251 140480029578304 decoder_stack.py:344] dstack: Final layernorm.
I0122 03:22:23.238691 140480029578304 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:37.535615 140480029578304 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/optim/base.py:49: DeprecationWarning: Use `optax` instead of `flax.optim`. Refer to the update guide https://flax.readthedocs.io/en/latest/howtos/optax_update_guide.html for detailed instructions.
  warnings.warn(
I0122 03:22:37.829998 140480029578304 training_loop.py:409] No working directory specified.
I0122 03:22:37.830088 140480029578304 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0122 03:22:37.830476 140480029578304 checkpoints.py:425] Found no checkpoint files in ag_ckpt_vocab with prefix checkpoint_
I0122 03:22:37.830563 140480029578304 training_loop.py:447] Only restoring trainable parameters.
I0122 03:22:37.831382 140480029578304 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0122 03:22:37.831473 140480029578304 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0122 03:22:37.831542 140480029578304 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:22:37.831604 140480029578304 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:22:37.831664 140480029578304 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.831727 140480029578304 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:22:37.831785 140480029578304 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.831843 140480029578304 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.831898 140480029578304 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:22:37.831942 140480029578304 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:22:37.831984 140480029578304 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.832026 140480029578304 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:22:37.832067 140480029578304 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:22:37.832109 140480029578304 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:22:37.832150 140480029578304 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.832192 140480029578304 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:22:37.832233 140480029578304 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.832274 140480029578304 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.832315 140480029578304 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:22:37.832356 140480029578304 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:22:37.832409 140480029578304 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.832451 140480029578304 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:22:37.832492 140480029578304 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:22:37.832533 140480029578304 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:22:37.832574 140480029578304 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.832615 140480029578304 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:22:37.832656 140480029578304 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.832698 140480029578304 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.832739 140480029578304 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:22:37.832780 140480029578304 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:22:37.832820 140480029578304 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.832862 140480029578304 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:22:37.832902 140480029578304 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:22:37.832945 140480029578304 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:22:37.833011 140480029578304 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.833074 140480029578304 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:22:37.833136 140480029578304 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.833182 140480029578304 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.833225 140480029578304 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:22:37.833266 140480029578304 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:22:37.833307 140480029578304 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.833348 140480029578304 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:22:37.833389 140480029578304 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:22:37.833431 140480029578304 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:22:37.833471 140480029578304 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.833512 140480029578304 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:22:37.833560 140480029578304 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.833601 140480029578304 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.833642 140480029578304 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:22:37.833683 140480029578304 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:22:37.833724 140480029578304 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.833765 140480029578304 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:22:37.833806 140480029578304 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:22:37.833847 140480029578304 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:22:37.833888 140480029578304 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.833929 140480029578304 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:22:37.833970 140480029578304 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.834011 140480029578304 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.834052 140480029578304 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:22:37.834092 140480029578304 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:22:37.834133 140480029578304 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.834174 140480029578304 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:22:37.834215 140480029578304 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:22:37.834256 140480029578304 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:22:37.834296 140480029578304 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.834337 140480029578304 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:22:37.834378 140480029578304 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.834419 140480029578304 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.834459 140480029578304 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:22:37.834499 140480029578304 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:22:37.834549 140480029578304 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.834592 140480029578304 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:22:37.834633 140480029578304 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:22:37.834679 140480029578304 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:22:37.834721 140480029578304 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.834762 140480029578304 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:22:37.834802 140480029578304 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.834843 140480029578304 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.834885 140480029578304 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:22:37.834925 140480029578304 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:22:37.834966 140480029578304 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.835006 140480029578304 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:22:37.835047 140480029578304 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:22:37.835088 140480029578304 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:22:37.835129 140480029578304 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.835170 140480029578304 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:22:37.835211 140480029578304 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.835252 140480029578304 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.835292 140480029578304 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:22:37.835333 140480029578304 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:22:37.835374 140480029578304 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.835415 140480029578304 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:22:37.835455 140480029578304 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:22:37.835496 140480029578304 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:22:37.835537 140480029578304 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.835578 140480029578304 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:22:37.835619 140480029578304 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.835660 140480029578304 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.835701 140480029578304 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:22:37.835742 140480029578304 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:22:37.835788 140480029578304 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.835831 140480029578304 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:22:37.835872 140480029578304 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:22:37.835913 140480029578304 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:22:37.835954 140480029578304 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.835995 140480029578304 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:22:37.836036 140480029578304 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.836077 140480029578304 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.836118 140480029578304 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:22:37.836159 140480029578304 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:22:37.836200 140480029578304 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.836242 140480029578304 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:22:37.836283 140480029578304 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:22:37.836324 140480029578304 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:22:37.836365 140480029578304 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.836406 140480029578304 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:22:37.836447 140480029578304 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.836488 140480029578304 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.836529 140480029578304 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:22:37.836570 140480029578304 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:22:37.836610 140480029578304 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:22:37.836651 140480029578304 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:22:37.836682 140480029578304 training_loop.py:725] Total parameters: 152072288
I0122 03:22:37.836841 140480029578304 training_loop.py:739] Total state size: 0
I0122 03:22:37.948440 140480029578304 training_loop.py:492] Training loop: creating task for mode beam_search
I0122 03:22:37.948645 140480029578304 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0122 03:22:37.949001 140480029578304 training_loop.py:652] Compiling mode beam_search with jit.
I0122 03:22:37.949335 140480029578304 training_loop.py:89] registering functions: dict_keys([])
I0122 03:22:37.953495 140480029578304 graph.py:498] translated_imo_2002_p2a
I0122 03:22:37.953587 140480029578304 graph.py:499] a b = segment a b; c = midpoint c a b; d = on_circle d c a; e = on_circle e c a, on_bline e d a; f = on_bline f c d, on_circle f c a; g = on_bline g c d, on_circle g c a; h = on_pline h c d e, on_line h d b ? eqangle f b f h f h f g
I0122 03:22:40.579252 140480029578304 ddar.py:60] Depth 1/1000 time = 2.5799672603607178
I0122 03:22:45.622503 140480029578304 ddar.py:60] Depth 2/1000 time = 5.043112754821777
I0122 03:22:51.826482 140480029578304 ddar.py:60] Depth 3/1000 time = 6.203776836395264
I0122 03:22:51.837416 140480029578304 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H : Points
CA = CB [00]
B,A,C are collinear [01]
CD = CA [02]
CE = CA [03]
ED = EA [04]
∠FCD = ∠CDF [05]
CF = CA [06]
FC = FD [07]
∠GCD = ∠CDG [08]
CG = CA [09]
GC = GD [10]
B,D,H are collinear [11]
HC ∥ DE [12]

 * Auxiliary Constructions:
: Points


 * Proof steps:
001. ∠GCD = ∠CDG [08] (Similar Triangles)⇒  CG:CD = DG:DC [13]
002. CG = CA [09] & CD = CA [02] ⇒  CD = CG [14]
003. ∠FCD = ∠CDF [05] (Similar Triangles)⇒  CF:CD = DF:DC [15]
004. CF:CD = DF:DC [15] & CF = CA [06] & CD = CA [02] ⇒  DF = DC [16]
005. CE = CA [03] & CD = CA [02] ⇒  CD = CE [17]
006. CD = CE [17] (SSS)⇒  ∠CDE = ∠DEC [18]
007. CD = CA [02] & ED = EA [04] ⇒  DA ⟂ CE [19]
008. CD = CA [02] & CA = CB [00] ⇒  C is the circumcenter of \Delta ADB [20]
009. C is the circumcenter of \Delta ADB [20] & B,A,C are collinear [01] ⇒  AD ⟂ BD [21]
010. B,D,H are collinear [11] & ∠CDE = ∠DEC [18] & DA ⟂ CE [19] & AD ⟂ BD [21] & DE ∥ CH [12] ⇒  ∠DCH = ∠CHD [22]
011. ∠DCH = ∠CHD [22] ⇒  DC = DH [23]
012. CG:CD = DG:DC [13] & CD = CG [14] & DF = DC [16] & DC = DH [23] ⇒  G,C,F,H are concyclic [24]
013. G,C,F,H are concyclic [24] ⇒  ∠GCH = ∠GFH [25]
014. GC = GD [10] & CG = CA [09] & CF = CA [06] ⇒  FC = DG [26]
015. DF = DC [16] & FC = DG [26] & CD = CG [14] (SSS)⇒  DF ∥ CG [27]
016. CD = CA [02] & CG = CA [09] & CE = CA [03] & CA = CB [00] & CF = CA [06] ⇒  F,E,D,G are concyclic [28]
017. F,E,D,G are concyclic [28] ⇒  ∠GFD = ∠GED [29]
018. GC = GD [10] & CG = CA [09] & CF = CA [06] & FC = FD [07] ⇒  DF = DG [30]
019. DF = DG [30] ⇒  ∠GFD = ∠DGF [31]
020. CD = CA [02] & CG = CA [09] & CE = CA [03] & CA = CB [00] & CF = CA [06] ⇒  F,D,B,G are concyclic [32]
021. F,D,B,G are concyclic [32] ⇒  ∠DBF = ∠DGF [33]
022. ∠GFD = ∠GED [29] & ∠GFD = ∠DGF [31] & ∠DBF = ∠DGF [33] & DA ⟂ CE [19] & AD ⟂ BD [21] ⇒  ∠(CE-BF) = ∠GED [34]
023. CE = CA [03] & CG = CA [09] ⇒  CG = CE [35]
024. CG = CE [35] ⇒  ∠CGE = ∠GEC [36]
025. ∠(CE-BF) = ∠GED [34] & ∠CGE = ∠GEC [36] ⇒  ∠(BF-EG) = ∠(DE-CG) [37]
026. CG = CA [09] & CF = CA [06] & FC = FD [07] ⇒  DF = GC [38]
027. B,D,H are collinear [11] & DE ∥ CH [12] & DA ⟂ CE [19] & AD ⟂ BD [21] ⇒  ∠EDH = ∠HCE [39]
028. B,D,H are collinear [11] & DA ⟂ CE [19] & AD ⟂ BD [21] ⇒  ∠EHD = ∠HEC [40]
029. ∠EDH = ∠HCE [39] & ∠EHD = ∠HEC [40] (Similar Triangles)⇒  HD = EC [41]
030. B,D,H are collinear [11] & DF ∥ CG [27] & DA ⟂ CE [19] & AD ⟂ BD [21] ⇒  ∠FDH = ∠GCE [42]
031. DF = GC [38] & HD = EC [41] & ∠FDH = ∠GCE [42] (SAS)⇒  ∠(DF-CG) = ∠(FH-EG) [43]
032. ∠GCH = ∠GFH [25] & DF ∥ CG [27] & CH ∥ DE [12] & ∠(BF-EG) = ∠(DE-CG) [37] & ∠(DF-CG) = ∠(FH-EG) [43] ⇒  ∠BFH = ∠HFG
==========================

I0122 03:22:51.837608 140480029578304 alphageometry.py:195] Solution written to ./output/imo_2002_p2a_solution.txt.
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
2024-01-22 03:22:55.935586: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
I0122 03:22:57.525186 139958912394304 inference_utils.py:69] Parsing gin configuration.
I0122 03:22:57.525276 139958912394304 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0122 03:22:57.525508 139958912394304 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0122 03:22:57.525561 139958912394304 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0122 03:22:57.525615 139958912394304 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0122 03:22:57.525669 139958912394304 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0122 03:22:57.525723 139958912394304 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0122 03:22:57.525773 139958912394304 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0122 03:22:57.525824 139958912394304 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0122 03:22:57.525875 139958912394304 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=2
I0122 03:22:57.525925 139958912394304 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0122 03:22:57.525975 139958912394304 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0122 03:22:57.526055 139958912394304 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0122 03:22:57.526248 139958912394304 resource_reader.py:55] Path not found: base_htrans.gin
I0122 03:22:57.526470 139958912394304 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0122 03:22:57.526651 139958912394304 resource_reader.py:55] Path not found: trainer_configuration.gin
I0122 03:22:57.533565 139958912394304 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0122 03:22:57.533752 139958912394304 resource_reader.py:55] Path not found: size/medium_150M.gin
I0122 03:22:57.534118 139958912394304 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0122 03:22:57.534250 139958912394304 resource_reader.py:55] Path not found: options/positions_t5.gin
I0122 03:22:57.534630 139958912394304 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0122 03:22:57.534761 139958912394304 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0122 03:22:57.535258 139958912394304 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0122 03:22:57.535387 139958912394304 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0122 03:22:57.539201 139958912394304 training_loop.py:334] ==== Training loop: initializing model ====
I0122 03:22:57.541588 139958912394304 xla_bridge.py:166] Remote TPU is not linked into jax; skipping remote TPU.
I0122 03:22:57.541662 139958912394304 xla_bridge.py:413] Unable to initialize backend 'tpu_driver': Could not initialize backend 'tpu_driver'
I0122 03:22:57.541747 139958912394304 xla_bridge.py:413] Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
I0122 03:22:57.541808 139958912394304 xla_bridge.py:413] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
I0122 03:22:57.542210 139958912394304 xla_bridge.py:413] Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.
I0122 03:22:57.542291 139958912394304 xla_bridge.py:413] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
W0122 03:22:57.542352 139958912394304 xla_bridge.py:420] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0122 03:22:57.542427 139958912394304 training_loop.py:335] Process 0 of 1
I0122 03:22:57.542482 139958912394304 training_loop.py:336] Local device count = 1
I0122 03:22:57.542562 139958912394304 training_loop.py:337] Number of replicas = 1
I0122 03:22:57.542613 139958912394304 training_loop.py:339] Using random number seed 42
I0122 03:22:57.754370 139958912394304 training_loop.py:359] Initializing the model.
I0122 03:22:57.849994 139958912394304 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.850230 139958912394304 decoder_stack.py:316] dstack: scanning over 1 windows.
I0122 03:22:57.850331 139958912394304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:22:57.850406 139958912394304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:22:57.850482 139958912394304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:22:57.850561 139958912394304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:22:57.850628 139958912394304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:22:57.850697 139958912394304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:22:57.850769 139958912394304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:22:57.850841 139958912394304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:22:57.850913 139958912394304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:22:57.850984 139958912394304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:22:57.851056 139958912394304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:22:57.851127 139958912394304 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:22:57.851181 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:57.851243 139958912394304 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0122 03:22:57.851378 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:57.851430 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:57.851482 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:57.853573 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.859349 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:57.874446 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.874925 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:57.879078 139958912394304 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:22:57.894786 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:57.894886 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:57.894933 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:57.894996 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.895103 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.896552 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.896684 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.897926 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.900790 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.908646 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.909634 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.909723 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:57.909776 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:57.909833 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.910094 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:57.910455 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:57.910516 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:57.915610 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.915797 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:57.921246 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.921379 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:57.921926 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:57.937650 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:57.949173 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.949303 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:57.949585 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.949669 139958912394304 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0122 03:22:57.949783 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:57.949827 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:57.949863 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:57.951595 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.953797 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:57.962655 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.963146 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:57.965759 139958912394304 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:22:57.970529 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:57.970619 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:57.970674 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:57.970737 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.970836 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.971270 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.971361 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.971815 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.972925 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.976155 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.976552 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.976642 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:57.976693 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:57.976772 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.977031 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:57.977374 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:57.977435 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:57.981974 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.982105 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:57.985308 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.985399 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:57.985717 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:57.990512 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:57.993539 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.993642 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:57.993986 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.994088 139958912394304 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0122 03:22:57.994221 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:57.994277 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:57.994330 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:57.996388 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:57.999341 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:58.008032 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.008452 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:58.010793 139958912394304 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:22:58.015057 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:58.015131 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:58.015181 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:58.015228 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.015305 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.015672 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.015758 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.016191 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.017309 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.019590 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.019932 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.020019 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:58.020056 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:58.020104 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.020308 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:58.020573 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:58.020620 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.023624 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.023707 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.025976 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.026051 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:58.026345 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:58.029649 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.033521 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.033642 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.033903 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.033987 139958912394304 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0122 03:22:58.034087 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:58.034129 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:58.034162 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:58.121862 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.124290 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:58.134032 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.134747 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:58.138502 139958912394304 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:22:58.143042 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:58.143116 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:58.143159 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:58.143206 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.143279 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.143584 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.143656 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.143968 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.144770 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.146979 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.147296 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.147371 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:58.147408 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:58.147457 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.147642 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:58.147891 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:58.147938 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.150839 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.150925 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.153191 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.153298 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:58.153588 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:58.156846 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.159800 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.159947 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.160333 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.160448 139958912394304 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0122 03:22:58.160614 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:58.160673 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:58.160726 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:58.162330 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.165631 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:58.175439 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.175933 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:58.178433 139958912394304 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:22:58.183226 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:58.183305 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:58.183346 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:58.183395 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.183488 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.183869 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.183943 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.184289 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.185072 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.187256 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.187590 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.187664 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:58.187711 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:58.187783 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.187983 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:58.188252 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:58.188305 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.191436 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.191529 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.193889 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.193966 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:58.194263 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:58.197828 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.200830 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.200927 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.201195 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.201291 139958912394304 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0122 03:22:58.201407 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:58.201457 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:58.201512 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:58.202913 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.205077 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:58.213750 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.214223 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:58.216601 139958912394304 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:22:58.220812 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:58.220887 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:58.220945 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:58.221002 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.221091 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.221432 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.221510 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.221852 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.222642 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.225221 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.225588 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.225669 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:58.225706 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:58.225756 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.225936 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:58.226185 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:58.226231 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.229504 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.229625 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.232006 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.232084 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:58.232380 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:58.235643 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.238514 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.238603 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.238843 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.238924 139958912394304 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0122 03:22:58.239016 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:58.239058 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:58.239091 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:58.240427 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.242625 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:58.251118 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.251520 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:58.253774 139958912394304 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:22:58.257918 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:58.257986 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:58.258026 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:58.258065 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.258129 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.258430 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.258510 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.258826 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.259632 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.261852 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.262156 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.262229 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:58.262273 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:58.262340 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.262549 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:58.262803 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:58.262856 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.265764 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.265857 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.268229 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.268313 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:58.268622 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:58.271871 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.274755 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.274841 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.275103 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.275185 139958912394304 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0122 03:22:58.275297 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:58.275341 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:58.275382 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:58.276752 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.279245 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:58.287764 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.288205 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:58.290535 139958912394304 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:22:58.294753 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:58.294826 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:58.294885 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:58.294928 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.295016 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.295348 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.295434 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.295773 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.296552 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.298729 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.299080 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.299169 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:58.299224 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:58.299281 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.299512 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:58.299764 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:58.299818 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.302726 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.302808 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.305140 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.305216 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:58.305510 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:58.308769 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.311625 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.311738 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.312027 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.312128 139958912394304 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0122 03:22:58.312245 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:58.312298 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:58.312349 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:58.313769 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.315941 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:58.324366 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.324826 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:58.327112 139958912394304 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:22:58.331614 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:58.331703 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:58.331768 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:58.331819 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.331905 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.332365 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.332457 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.332905 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.333998 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.336204 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.336556 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.336644 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:58.336696 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:58.336749 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.336989 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:58.337241 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:58.337295 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.340221 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.340324 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.342964 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.343079 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:58.343412 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:58.347155 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.350122 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.350221 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.350534 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.350618 139958912394304 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0122 03:22:58.350738 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:58.350783 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:58.350823 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:58.352210 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.354378 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:58.363029 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.363624 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:58.366088 139958912394304 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:22:58.370445 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:58.370537 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:58.370599 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:58.370660 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.370743 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.371073 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.371153 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.371491 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.372265 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.374488 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.374846 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.374931 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:58.374984 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:58.375060 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.375265 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:58.375538 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:58.375592 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.378547 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.378649 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.382030 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.382156 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:58.382498 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:58.385838 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.389112 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.389217 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.389489 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.389587 139958912394304 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0122 03:22:58.389700 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:58.389750 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:58.389803 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:58.391204 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.393429 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:58.402479 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.402948 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:58.405216 139958912394304 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:22:58.409529 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:58.409594 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:58.409654 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:58.409716 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.409796 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.410235 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.410340 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.410811 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.411875 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.414485 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.414924 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.415015 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:58.415071 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:58.415148 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.415360 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:58.415671 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:58.415731 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.418716 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.418808 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.421116 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.421202 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:58.421516 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:58.424798 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.427710 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.427804 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.428079 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.428178 139958912394304 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0122 03:22:58.428303 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:58.428354 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:58.428410 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:58.430305 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.432809 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:58.441431 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.441887 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:58.444659 139958912394304 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:22:58.449810 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:58.449909 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:58.449971 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:58.450033 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.450125 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.450510 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.450598 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.451044 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.452147 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.454329 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.454663 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.454744 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:58.454801 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:58.454878 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.455081 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:58.455339 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:58.455393 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.458335 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.458445 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.460741 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.460829 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:58.461153 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:58.465198 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.468291 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.468401 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.468680 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.468936 139958912394304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:22:58.469005 139958912394304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:22:58.469074 139958912394304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:22:58.469137 139958912394304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:22:58.469200 139958912394304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:22:58.469261 139958912394304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:22:58.469322 139958912394304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:22:58.469384 139958912394304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:22:58.469446 139958912394304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:22:58.469508 139958912394304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:22:58.469569 139958912394304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:22:58.469631 139958912394304 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:22:58.469687 139958912394304 decoder_stack.py:344] dstack: Final layernorm.
I0122 03:22:58.474226 139958912394304 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:22:58.539628 139958912394304 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.539756 139958912394304 decoder_stack.py:333] dstack: autoregressive generator.
I0122 03:22:58.539825 139958912394304 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0122 03:22:58.539946 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:58.539997 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:58.540052 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:58.540141 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.542298 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:58.552134 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.552612 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:58.554906 139958912394304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:58.571262 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:58.571354 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:58.571420 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:58.571483 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.571574 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.572477 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.572559 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.573301 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.575339 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.580315 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.581306 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.581397 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:58.581455 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:58.581539 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.581743 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:58.581863 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:58.581913 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.585222 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.585316 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.587472 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.587556 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:58.587679 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:58.591494 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.594545 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.594640 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.594913 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.595006 139958912394304 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0122 03:22:58.595126 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:58.595176 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:58.595231 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:58.595309 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.598089 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:58.611386 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.611889 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:58.615507 139958912394304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:58.629150 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:58.629276 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:58.629348 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:58.629406 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.629513 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.629989 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.630093 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.630661 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.631749 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.634552 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.634910 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.634987 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:58.635034 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:58.635113 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.635315 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:58.635428 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:58.635478 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.639629 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.639718 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.641917 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.641996 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:58.642118 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:58.645694 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.649883 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.650007 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.650364 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.650472 139958912394304 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0122 03:22:58.650604 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:58.650650 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:58.650691 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:58.650763 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.652801 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:58.662504 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.663019 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:58.667648 139958912394304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:58.685606 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:58.685691 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:58.685734 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:58.685786 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.685893 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.686329 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.686419 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.686880 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.687796 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.690033 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.690407 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.690498 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:58.690552 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:58.690609 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.690815 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:58.690932 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:58.690987 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.695260 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.695355 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.697719 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.697826 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:58.697945 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:58.701577 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.704606 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.704693 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.704949 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.705033 139958912394304 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0122 03:22:58.705137 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:58.705180 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:58.705214 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:58.705260 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.707294 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:58.716730 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.717194 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:58.719451 139958912394304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:58.731546 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:58.731643 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:58.731685 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:58.731733 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.731818 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.732220 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.732309 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.732769 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.733723 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.735983 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.736309 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.736384 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:58.736421 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:58.736482 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.736682 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:58.736788 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:58.736843 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.740461 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.740544 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.743270 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.743352 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:58.743462 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:58.747420 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.750605 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.750703 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.750964 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.751049 139958912394304 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0122 03:22:58.751158 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:58.751201 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:58.751234 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:58.751281 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.754049 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:58.763600 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.764554 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:58.768519 139958912394304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:58.780597 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:58.780685 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:58.780728 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:58.780767 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.780848 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.781172 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.781244 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.781574 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.782273 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.784517 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.784836 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.784912 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:58.784950 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:58.785001 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.785193 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:58.785290 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:58.785336 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.788518 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.788601 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.790761 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.790839 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:58.790938 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:58.794548 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.798096 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.798220 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.798557 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.798645 139958912394304 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0122 03:22:58.798759 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:58.798802 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:58.798836 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:58.798883 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.800941 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:58.809942 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.810356 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:58.812641 139958912394304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:58.825737 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:58.825821 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:58.825864 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:58.825903 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.825984 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.826393 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.826512 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.826994 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.827687 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.830364 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.830777 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.830861 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:58.830899 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:58.830952 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.831161 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:58.831268 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:58.831323 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.834557 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.834641 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.836766 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.836843 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:58.836941 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:58.840518 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.843605 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.843695 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.843949 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.844033 139958912394304 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0122 03:22:58.844137 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:58.844181 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:58.844215 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:58.844263 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.846395 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:58.866582 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.867536 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:58.869980 139958912394304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:58.882289 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:58.882374 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:58.882416 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:58.882455 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.882550 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.882878 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.882961 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.883291 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.883961 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.886170 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.886544 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.886620 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:58.886658 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:58.886709 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.886897 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:58.886995 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:58.887041 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.890183 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.890266 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.892430 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.892507 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:58.892610 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:58.896067 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.899586 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.899708 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.899983 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.900069 139958912394304 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0122 03:22:58.900172 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:58.900216 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:58.900250 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:58.900297 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.902953 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:58.911942 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.912358 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:58.914897 139958912394304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:58.927024 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:58.927123 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:58.927169 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:58.927221 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.927307 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.927722 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.927814 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.928283 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.929265 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.932644 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.933096 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.933192 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:58.933250 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:58.933324 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.933602 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:58.933737 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:58.933795 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.938572 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.938662 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.940793 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.940876 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:58.941003 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:58.946235 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.949583 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.949698 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.949943 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.950036 139958912394304 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0122 03:22:58.950152 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:58.950204 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:58.950255 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:58.950326 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.953151 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:58.962275 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.962820 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:58.965359 139958912394304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:58.977222 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:58.977297 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:58.977339 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:58.977391 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.977477 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.977799 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.977881 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.978244 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.979048 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.981700 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.982079 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.982162 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:58.982200 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:58.982252 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.982444 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:58.982566 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:58.982609 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.985808 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.985894 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.988020 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.988098 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:58.988189 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:58.991773 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:58.994896 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.995018 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:58.995318 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.995411 139958912394304 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0122 03:22:58.995519 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:58.995562 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:58.995604 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:58.995693 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:58.998767 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:59.007873 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.008301 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:59.010540 139958912394304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:59.023069 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:59.023154 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:59.023197 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:59.023254 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.023336 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.023652 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.023725 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.024047 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.024738 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.027134 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.027463 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.027538 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:59.027575 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:59.027625 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.027875 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:59.028003 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:59.028061 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:59.031952 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.032071 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:59.034224 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.034303 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:59.034395 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:59.037785 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:59.040792 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.040875 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:59.041114 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.041198 139958912394304 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0122 03:22:59.041290 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:59.041332 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:59.041365 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:59.041412 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.043372 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:59.053998 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.054430 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:59.056652 139958912394304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:59.069152 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:59.069235 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:59.069278 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:59.069319 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.069403 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.069712 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.069784 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.070097 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.070764 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.072949 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.073268 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.073344 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:59.073381 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:59.073432 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.073628 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:59.073731 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:59.073775 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:59.077015 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.077099 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:59.079634 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.079737 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:59.079876 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:59.084276 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:59.090633 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.090776 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:59.091225 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.091359 139958912394304 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0122 03:22:59.091519 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:59.091591 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:59.091653 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:59.091741 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.095585 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:59.108050 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.108528 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:59.110834 139958912394304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:59.126211 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:59.126319 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:59.126388 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:59.126461 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.126612 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.127125 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.127254 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.127836 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.129062 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.132644 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.133175 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.133292 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:59.133343 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:59.133415 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.133693 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:59.133834 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:59.133889 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:59.138812 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.138992 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:59.142416 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.142587 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:59.142746 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:59.148298 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:59.153121 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.153298 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:59.153680 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.153818 139958912394304 decoder_stack.py:344] dstack: Final layernorm.
I0122 03:22:59.157723 139958912394304 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:22:59.232073 139958912394304 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.232209 139958912394304 decoder_stack.py:333] dstack: autoregressive generator.
I0122 03:22:59.232285 139958912394304 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0122 03:22:59.232396 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:59.232457 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:59.232523 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:59.232597 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.236057 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:59.245680 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.246129 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:59.248358 139958912394304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:59.259876 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:59.259943 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:59.260005 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:59.260066 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.260149 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.260478 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.260557 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.260894 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.261560 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.263741 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.264065 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.264147 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:59.264203 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:59.264278 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.264475 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:59.264588 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:59.264638 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:59.267720 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.267812 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:59.270730 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.270813 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:59.270931 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:59.274335 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:59.277382 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.277476 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:59.277740 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.277835 139958912394304 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0122 03:22:59.277947 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:59.277997 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:59.278049 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:59.278121 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.280907 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:59.291436 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.291913 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:59.294205 139958912394304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:59.306901 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:59.307013 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:59.307079 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:59.307141 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.307254 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.307738 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.307837 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.308332 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.309335 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.311931 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.312271 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.312355 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:59.312409 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:59.312485 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.312685 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:59.312813 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:59.312869 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:59.316450 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.316544 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:59.318687 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.318773 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:59.318888 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:59.322284 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:59.325415 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.325512 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:59.325777 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.325873 139958912394304 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0122 03:22:59.325984 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:59.326034 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:59.326088 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:59.326165 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.328255 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:59.337232 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.337655 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:59.339885 139958912394304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:59.351323 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:59.351399 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:59.351473 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:59.351536 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.351621 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.351949 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.352030 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.352371 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.353037 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.355502 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.355828 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.355910 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:59.355964 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:59.356040 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.356238 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:59.356350 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:59.356401 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:59.359527 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.359619 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:59.361756 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.361839 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:59.361956 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:59.365368 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:59.368421 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.368517 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:59.368783 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.368879 139958912394304 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0122 03:22:59.368990 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:59.369039 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:59.369091 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:59.369163 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.371940 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:59.380900 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.381330 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:59.383547 139958912394304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:59.394957 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:59.395035 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:59.395094 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:59.395164 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.395245 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.395570 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.395648 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.395984 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.396643 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.398844 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.399188 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.399285 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:59.399338 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:59.399411 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.399648 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:59.399775 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:59.399826 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:59.402928 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.403018 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:59.405142 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.405224 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:59.405340 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:59.408739 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:59.411986 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.412077 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:59.412339 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.412430 139958912394304 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0122 03:22:59.412540 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:59.412589 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:59.412638 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:59.412709 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.415506 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:59.424504 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.424922 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:59.427137 139958912394304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:59.439001 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:59.439098 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:59.439166 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:59.439226 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.439323 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.439662 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.439740 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.440086 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.440768 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.442979 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.443309 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.443390 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:59.443444 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:59.443519 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.443722 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:59.443832 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:59.443881 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:59.447524 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.447664 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:59.449937 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.450055 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:59.450184 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:59.453811 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:59.457019 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.457147 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:59.457424 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.457523 139958912394304 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0122 03:22:59.457634 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:59.457682 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:59.457733 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:59.457805 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.460497 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:59.470008 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.470473 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:59.472723 139958912394304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:59.484423 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:59.484509 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:59.484575 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:59.484634 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.484752 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.485185 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.485273 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.485734 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.486648 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.488869 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.489222 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.489315 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:59.489371 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:59.489445 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.489724 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:59.489851 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:59.489900 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:59.494577 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.494667 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:59.496816 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.496906 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:59.497024 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:59.502468 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:59.505597 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.505699 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:59.505965 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.506060 139958912394304 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0122 03:22:59.506188 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:59.506241 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:59.506292 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:59.506367 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.508379 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:59.517355 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.517775 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:59.520003 139958912394304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:59.531651 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:59.531734 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:59.531795 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:59.531856 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.531939 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.532293 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.532383 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.532720 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.533386 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.536049 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.536421 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.536517 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:59.536570 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:59.536644 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.536857 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:59.536983 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:59.537032 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:59.540197 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.540287 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:59.542408 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.542489 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:59.542608 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:59.546178 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:59.549460 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.549597 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:59.549880 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.549982 139958912394304 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0122 03:22:59.550105 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:59.550155 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:59.550201 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:59.550283 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.552883 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:59.562275 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.562758 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:59.565343 139958912394304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:59.577364 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:59.577441 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:59.577483 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:59.577521 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.577595 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.577918 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.577990 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.578307 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.578977 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.581906 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.582396 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.582510 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:59.582566 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:59.582641 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.582921 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:59.583054 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:59.583097 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:59.587661 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.587793 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:59.591820 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.591938 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:59.592090 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:59.598262 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:59.602283 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.602462 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:59.602949 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.603081 139958912394304 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0122 03:22:59.603243 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:59.603311 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:59.603371 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:59.603457 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.607322 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:59.624573 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.625191 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:59.627908 139958912394304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:59.640589 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:59.640673 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:59.640725 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:59.640778 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.640868 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.641203 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.641283 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.641730 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.642427 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.644731 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.645079 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.645155 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:59.645200 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:59.645276 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.645483 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:59.645596 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:59.645645 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:59.649235 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.649358 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:59.651603 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.651685 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:59.651790 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:59.655232 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:59.658276 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.658359 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:59.658655 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.658738 139958912394304 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0122 03:22:59.658860 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:59.658903 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:59.658942 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:59.659008 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.661000 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:59.672064 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.672586 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:59.674931 139958912394304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:59.687593 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:59.687676 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:59.687728 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:59.687784 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.687871 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.688215 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.688293 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.688641 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.689316 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.691602 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.691934 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.692009 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:59.692057 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:59.692128 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.692326 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:59.692438 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:59.692487 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:59.695700 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.695798 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:59.699061 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.699212 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:59.699353 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:59.703659 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:59.706724 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.706812 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:59.707072 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.707154 139958912394304 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0122 03:22:59.707264 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:59.707306 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:59.707345 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:59.707415 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.709438 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:59.719921 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.720383 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:59.722627 139958912394304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:59.735174 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:59.735258 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:59.735307 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:59.735362 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.735451 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.735852 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.735939 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.736414 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.737349 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.740133 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.740463 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.740537 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:59.740586 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:59.740658 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.740856 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:59.740970 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:59.741020 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:59.847245 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.847425 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:59.850366 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.850484 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:59.850594 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:59.854182 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:59.857337 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.857432 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:59.857675 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.857758 139958912394304 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0122 03:22:59.857848 139958912394304 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:22:59.857888 139958912394304 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:22:59.857920 139958912394304 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:22:59.857967 139958912394304 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.859969 139958912394304 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:22:59.872224 139958912394304 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.872698 139958912394304 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:22:59.875068 139958912394304 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:22:59.889461 139958912394304 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:22:59.889547 139958912394304 attention.py:418] Single window, no scan.
I0122 03:22:59.889591 139958912394304 transformer_layer.py:389] tlayer: self-attention.
I0122 03:22:59.889629 139958912394304 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.889704 139958912394304 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.890011 139958912394304 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.890083 139958912394304 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.890465 139958912394304 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.891186 139958912394304 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.893349 139958912394304 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.893668 139958912394304 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.893742 139958912394304 transformer_layer.py:468] tlayer: End windows.
I0122 03:22:59.893777 139958912394304 transformer_layer.py:472] tlayer: final FFN.
I0122 03:22:59.893826 139958912394304 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.894012 139958912394304 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:22:59.894110 139958912394304 nn_components.py:325] mlp: activation = None
I0122 03:22:59.894161 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:59.898937 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.899065 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:59.901278 139958912394304 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.901357 139958912394304 transformer_base.py:443] tbase: final FFN
I0122 03:22:59.901453 139958912394304 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:22:59.904957 139958912394304 nn_components.py:329] mlp: final activation = None
I0122 03:22:59.908110 139958912394304 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.908205 139958912394304 nn_components.py:261] mlp: residual
I0122 03:22:59.908458 139958912394304 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:22:59.908545 139958912394304 decoder_stack.py:344] dstack: Final layernorm.
I0122 03:22:59.911683 139958912394304 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:13.954422 139958912394304 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/optim/base.py:49: DeprecationWarning: Use `optax` instead of `flax.optim`. Refer to the update guide https://flax.readthedocs.io/en/latest/howtos/optax_update_guide.html for detailed instructions.
  warnings.warn(
I0122 03:23:14.269348 139958912394304 training_loop.py:409] No working directory specified.
I0122 03:23:14.269446 139958912394304 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0122 03:23:14.269890 139958912394304 checkpoints.py:425] Found no checkpoint files in ag_ckpt_vocab with prefix checkpoint_
I0122 03:23:14.269978 139958912394304 training_loop.py:447] Only restoring trainable parameters.
I0122 03:23:14.270690 139958912394304 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0122 03:23:14.270763 139958912394304 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0122 03:23:14.270823 139958912394304 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:23:14.270878 139958912394304 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:23:14.270941 139958912394304 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.271014 139958912394304 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:23:14.271084 139958912394304 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.271163 139958912394304 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.271232 139958912394304 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:23:14.271300 139958912394304 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:23:14.271368 139958912394304 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.271438 139958912394304 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:23:14.271507 139958912394304 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:23:14.271574 139958912394304 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:23:14.271642 139958912394304 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.271710 139958912394304 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:23:14.271779 139958912394304 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.271848 139958912394304 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.271916 139958912394304 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:23:14.271984 139958912394304 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:23:14.272053 139958912394304 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.272122 139958912394304 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:23:14.272189 139958912394304 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:23:14.272257 139958912394304 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:23:14.272328 139958912394304 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.272397 139958912394304 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:23:14.272464 139958912394304 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.272531 139958912394304 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.272595 139958912394304 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:23:14.272660 139958912394304 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:23:14.272724 139958912394304 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.272788 139958912394304 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:23:14.272852 139958912394304 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:23:14.272932 139958912394304 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:23:14.272997 139958912394304 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.273061 139958912394304 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:23:14.273125 139958912394304 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.273189 139958912394304 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.273253 139958912394304 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:23:14.273317 139958912394304 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:23:14.273381 139958912394304 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.273444 139958912394304 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:23:14.273508 139958912394304 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:23:14.273571 139958912394304 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:23:14.273635 139958912394304 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.273699 139958912394304 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:23:14.273762 139958912394304 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.273826 139958912394304 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.273890 139958912394304 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:23:14.273954 139958912394304 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:23:14.274018 139958912394304 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.274081 139958912394304 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:23:14.274146 139958912394304 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:23:14.274209 139958912394304 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:23:14.274273 139958912394304 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.274337 139958912394304 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:23:14.274400 139958912394304 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.274464 139958912394304 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.274534 139958912394304 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:23:14.274599 139958912394304 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:23:14.274669 139958912394304 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.274733 139958912394304 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:23:14.274797 139958912394304 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:23:14.274861 139958912394304 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:23:14.274924 139958912394304 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.274989 139958912394304 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:23:14.275054 139958912394304 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.275126 139958912394304 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.275194 139958912394304 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:23:14.275263 139958912394304 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:23:14.275330 139958912394304 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.275394 139958912394304 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:23:14.275458 139958912394304 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:23:14.275522 139958912394304 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:23:14.275586 139958912394304 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.275650 139958912394304 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:23:14.275714 139958912394304 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.275778 139958912394304 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.275844 139958912394304 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:23:14.275913 139958912394304 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:23:14.275980 139958912394304 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.276046 139958912394304 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:23:14.276111 139958912394304 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:23:14.276174 139958912394304 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:23:14.276233 139958912394304 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.276294 139958912394304 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:23:14.276353 139958912394304 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.276420 139958912394304 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.276483 139958912394304 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:23:14.276542 139958912394304 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:23:14.276600 139958912394304 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.276662 139958912394304 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:23:14.276720 139958912394304 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:23:14.276786 139958912394304 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:23:14.276855 139958912394304 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.276915 139958912394304 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:23:14.276973 139958912394304 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.277031 139958912394304 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.277088 139958912394304 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:23:14.277145 139958912394304 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:23:14.277208 139958912394304 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.277272 139958912394304 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:23:14.277330 139958912394304 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:23:14.277386 139958912394304 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:23:14.277443 139958912394304 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.277500 139958912394304 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:23:14.277557 139958912394304 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.277614 139958912394304 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.277677 139958912394304 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:23:14.277745 139958912394304 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:23:14.277811 139958912394304 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.277876 139958912394304 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:23:14.277941 139958912394304 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0122 03:23:14.278013 139958912394304 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0122 03:23:14.278079 139958912394304 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.278144 139958912394304 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0122 03:23:14.278210 139958912394304 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.278275 139958912394304 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.278340 139958912394304 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0122 03:23:14.278406 139958912394304 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0122 03:23:14.278471 139958912394304 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0122 03:23:14.278544 139958912394304 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0122 03:23:14.278590 139958912394304 training_loop.py:725] Total parameters: 152072288
I0122 03:23:14.278826 139958912394304 training_loop.py:739] Total state size: 0
I0122 03:23:14.400341 139958912394304 training_loop.py:492] Training loop: creating task for mode beam_search
I0122 03:23:14.400544 139958912394304 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0122 03:23:14.400825 139958912394304 training_loop.py:652] Compiling mode beam_search with jit.
I0122 03:23:14.401083 139958912394304 training_loop.py:89] registering functions: dict_keys([])
I0122 03:23:14.405353 139958912394304 graph.py:498] translated_imo_2002_p2b
I0122 03:23:14.405514 139958912394304 graph.py:499] a b = segment a b; c = midpoint c a b; d = on_circle d c a; e = on_circle e c a, on_bline e d a; f = on_bline f c d, on_circle f c a; g = on_bline g c d, on_circle g c a; h = on_pline h c d e, on_line h d b ? eqangle b f b h b h b g
I0122 03:23:16.982853 139958912394304 ddar.py:60] Depth 1/1000 time = 2.5242433547973633
I0122 03:23:16.991657 139958912394304 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H : Points
CA = CB [00]
CD = CA [01]
CE = CA [02]
CF = CA [03]
FC = FD [04]
CG = CA [05]
GC = GD [06]
D,B,H are collinear [07]

 * Auxiliary Constructions:
: Points


 * Proof steps:
001. CF = CA [03] & CG = CA [05] & CE = CA [02] & CD = CA [01] & CA = CB [00] ⇒  D,B,F,G are concyclic [08]
002. D,B,F,G are concyclic [08] ⇒  ∠DBG = ∠DFG [09]
003. D,B,F,G are concyclic [08] ⇒  ∠FBD = ∠FGD [10]
004. FC = FD [04] & CF = CA [03] & CG = CA [05] & GC = GD [06] ⇒  DG = DF [11]
005. DG = DF [11] ⇒  ∠FGD = ∠DFG [12]
006. D,B,H are collinear [07] & ∠DBG = ∠DFG [09] & ∠FGD = ∠DFG [12] & ∠FBD = ∠FGD [10] ⇒  ∠FBH = ∠HBG
==========================

I0122 03:23:16.991833 139958912394304 alphageometry.py:195] Solution written to ./output/imo_2002_p2b_solution.txt.
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
2024-01-22 03:23:21.400289: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/zhangzs/anaconda3/envs/geo/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
I0122 03:23:22.973046 140358405149760 inference_utils.py:69] Parsing gin configuration.
I0122 03:23:22.973140 140358405149760 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0122 03:23:22.973331 140358405149760 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0122 03:23:22.973367 140358405149760 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0122 03:23:22.973399 140358405149760 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0122 03:23:22.973430 140358405149760 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0122 03:23:22.973460 140358405149760 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0122 03:23:22.973488 140358405149760 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0122 03:23:22.973517 140358405149760 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0122 03:23:22.973546 140358405149760 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=2
I0122 03:23:22.973573 140358405149760 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0122 03:23:22.973601 140358405149760 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0122 03:23:22.973650 140358405149760 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0122 03:23:22.973776 140358405149760 resource_reader.py:55] Path not found: base_htrans.gin
I0122 03:23:22.973939 140358405149760 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0122 03:23:22.974048 140358405149760 resource_reader.py:55] Path not found: trainer_configuration.gin
I0122 03:23:22.979408 140358405149760 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0122 03:23:22.979534 140358405149760 resource_reader.py:55] Path not found: size/medium_150M.gin
I0122 03:23:22.979903 140358405149760 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0122 03:23:22.980035 140358405149760 resource_reader.py:55] Path not found: options/positions_t5.gin
I0122 03:23:22.980362 140358405149760 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0122 03:23:22.980490 140358405149760 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0122 03:23:22.981002 140358405149760 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0122 03:23:22.981134 140358405149760 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0122 03:23:22.984853 140358405149760 training_loop.py:334] ==== Training loop: initializing model ====
I0122 03:23:22.986974 140358405149760 xla_bridge.py:166] Remote TPU is not linked into jax; skipping remote TPU.
I0122 03:23:22.987036 140358405149760 xla_bridge.py:413] Unable to initialize backend 'tpu_driver': Could not initialize backend 'tpu_driver'
I0122 03:23:22.987104 140358405149760 xla_bridge.py:413] Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
I0122 03:23:22.987152 140358405149760 xla_bridge.py:413] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
I0122 03:23:22.987530 140358405149760 xla_bridge.py:413] Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.
I0122 03:23:22.987595 140358405149760 xla_bridge.py:413] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
W0122 03:23:22.987640 140358405149760 xla_bridge.py:420] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0122 03:23:22.987690 140358405149760 training_loop.py:335] Process 0 of 1
I0122 03:23:22.987728 140358405149760 training_loop.py:336] Local device count = 1
I0122 03:23:22.987768 140358405149760 training_loop.py:337] Number of replicas = 1
I0122 03:23:22.987800 140358405149760 training_loop.py:339] Using random number seed 42
I0122 03:23:23.181861 140358405149760 training_loop.py:359] Initializing the model.
I0122 03:23:23.272872 140358405149760 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.273102 140358405149760 decoder_stack.py:316] dstack: scanning over 1 windows.
I0122 03:23:23.273173 140358405149760 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:23:23.273257 140358405149760 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:23:23.273303 140358405149760 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:23:23.273347 140358405149760 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:23:23.273390 140358405149760 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:23:23.273432 140358405149760 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:23:23.273474 140358405149760 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:23:23.273516 140358405149760 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:23:23.273557 140358405149760 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:23:23.273598 140358405149760 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:23:23.273640 140358405149760 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:23:23.273682 140358405149760 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0122 03:23:23.273718 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:23.273756 140358405149760 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0122 03:23:23.273845 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:23.273884 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:23.273916 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:23.275346 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.280741 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:23.294842 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.295354 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:23.299669 140358405149760 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:23:23.313479 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:23.313575 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:23.313623 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:23.313684 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.313770 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.315039 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.315137 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.315863 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.318399 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.324585 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.325532 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.325627 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:23.325668 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:23.325735 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.325965 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:23.326332 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:23.326396 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:23.329874 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.330012 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:23.332931 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.333061 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:23.333384 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:23.345528 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:23.356114 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.356240 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:23.356503 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.356597 140358405149760 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0122 03:23:23.356739 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:23.356819 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:23.356874 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:23.358963 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.361119 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:23.369666 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.370123 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:23.372452 140358405149760 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:23:23.377006 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:23.377074 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:23.377137 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:23.377200 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.377295 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.377633 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.377713 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.378162 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.379037 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.381231 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.381552 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.381633 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:23.381691 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:23.381768 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.381970 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:23.382224 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:23.382278 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:23.385379 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.385469 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:23.387755 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.387838 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:23.388156 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:23.391643 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:23.394628 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.394721 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:23.394995 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.395089 140358405149760 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0122 03:23:23.395209 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:23.395259 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:23.395313 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:23.396686 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.398937 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:23.409192 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.409922 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:23.413835 140358405149760 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:23:23.419500 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:23.419597 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:23.419641 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:23.419691 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.419776 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.420186 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.420269 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.420771 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.421910 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.425044 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.425403 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.425480 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:23.425528 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:23.425603 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.425805 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:23.426067 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:23.426120 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:23.429177 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.429263 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:23.431541 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.431624 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:23.431934 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:23.435225 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:23.438463 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.438568 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:23.438840 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.438924 140358405149760 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0122 03:23:23.439035 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:23.439078 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:23.439119 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:23.527244 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.529912 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:23.538413 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.538851 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:23.541079 140358405149760 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:23:23.545273 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:23.545344 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:23.545384 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:23.545424 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.545489 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.545797 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.545869 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.546177 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.546977 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.549471 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.549812 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.549924 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:23.549984 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:23.550069 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.550395 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:23.550791 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:23.550855 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:23.555286 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.555409 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:23.558351 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.558561 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:23.558968 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:23.563173 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:23.566582 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.566729 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:23.567064 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.567182 140358405149760 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0122 03:23:23.567332 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:23.567384 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:23.567441 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:23.569666 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.572920 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:23.582745 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.583386 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:23.585711 140358405149760 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:23:23.590556 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:23.590657 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:23.590721 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:23.590784 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.590881 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.591271 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.591360 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.591715 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.592557 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.595332 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.595830 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.595925 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:23.595983 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:23.596068 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.596284 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:23.596559 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:23.596613 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:23.599658 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.599752 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:23.602085 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.602171 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:23.602581 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:23.605906 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:23.608827 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.608922 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:23.609194 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.609288 140358405149760 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0122 03:23:23.609407 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:23.609457 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:23.609512 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:23.611113 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.613544 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:23.622194 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.622711 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:23.625031 140358405149760 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:23:23.629271 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:23.629339 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:23.629404 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:23.629469 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.629556 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.629885 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.629965 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.630318 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.631153 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.633314 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.633647 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.633729 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:23.633786 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:23.633863 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.634065 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:23.634314 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:23.634367 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:23.637596 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.637690 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:23.640149 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.640233 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:23.640552 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:23.643806 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:23.646716 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.646809 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:23.647080 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.647173 140358405149760 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0122 03:23:23.647293 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:23.647343 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:23.647398 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:23.648765 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.650975 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:23.659399 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.659821 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:23.662166 140358405149760 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:23:23.666344 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:23.666426 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:23.666491 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:23.666566 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.666678 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.667011 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.667091 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.667429 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.668250 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.670386 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.670731 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.670829 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:23.670885 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:23.670961 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.671164 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:23.671415 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:23.671467 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:23.674340 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.674429 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:23.676777 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.676860 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:23.677172 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:23.680414 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:23.683296 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.683387 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:23.683657 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.683750 140358405149760 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0122 03:23:23.683868 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:23.683918 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:23.683973 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:23.685345 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.687816 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:23.696596 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.697060 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:23.699377 140358405149760 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:23:23.703541 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:23.703618 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:23.703681 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:23.703745 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.703831 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.704169 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.704249 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.704586 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.705365 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.707545 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.707895 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.707989 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:23.708047 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:23.708124 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.708369 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:23.708641 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:23.708695 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:23.711605 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.711695 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:23.714016 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.714101 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:23.714509 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:23.717749 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:23.720571 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.720662 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:23.720932 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.721024 140358405149760 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0122 03:23:23.721143 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:23.721193 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:23.721248 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:23.722679 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.724818 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:23.733260 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.733733 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:23.736003 140358405149760 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:23:23.740499 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:23.740567 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:23.740631 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:23.740694 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.740781 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.741108 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.741188 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.741528 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.742291 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.744454 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.744785 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.744866 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:23.744922 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:23.744998 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.745238 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:23.745494 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:23.745549 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:23.748464 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.748555 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:23.750891 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.750975 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:23.751291 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:23.754543 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:23.757368 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.757457 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:23.757767 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.757862 140358405149760 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0122 03:23:23.757982 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:23.758031 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:23.758086 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:23.759580 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.761705 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:23.770177 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.770688 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:23.772951 140358405149760 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:23:23.777163 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:23.777231 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:23.777295 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:23.777358 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.777442 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.777768 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.777848 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.778290 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.779134 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.781331 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.781668 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.781750 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:23.781807 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:23.781886 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.782089 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:23.782363 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:23.782417 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:23.785327 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.785416 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:23.787969 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.788069 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:23.788394 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:23.791691 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:23.794985 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.795085 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:23.795360 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.795454 140358405149760 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0122 03:23:23.795573 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:23.795623 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:23.795678 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:23.797060 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.799277 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:23.808554 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.809030 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:23.811299 140358405149760 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:23:23.815940 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:23.816038 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:23.816105 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:23.816171 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.816264 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.816613 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.816694 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.817044 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.818383 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.821824 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.822239 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.822342 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:23.822405 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:23.822490 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.822735 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:23.823036 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:23.823094 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:23.826338 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.826459 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:23.828794 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.828887 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:23.829232 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:23.832786 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:23.835918 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.836057 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:23.836353 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.836453 140358405149760 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0122 03:23:23.836577 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:23.836628 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:23.836684 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:23.838138 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.840542 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:23.849540 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.849982 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:23.852609 140358405149760 transformer_layer.py:213] tlayer: windowed attention.
I0122 03:23:23.856822 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:23.856887 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:23.856928 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:23.856967 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.857049 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.857427 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.857508 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.857850 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.858645 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.860800 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.861124 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.861213 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:23.861270 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:23.861348 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.861602 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:23.861869 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:23.861923 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:23.864848 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.864940 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:23.867352 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.867444 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:23.867778 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:23.871185 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:23.874398 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.874541 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:23.874831 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.875100 140358405149760 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:23:23.875171 140358405149760 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:23:23.875239 140358405149760 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:23:23.875303 140358405149760 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:23:23.875367 140358405149760 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:23:23.875429 140358405149760 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:23:23.875491 140358405149760 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:23:23.875553 140358405149760 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:23:23.875616 140358405149760 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:23:23.875679 140358405149760 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:23:23.875741 140358405149760 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:23:23.875803 140358405149760 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0122 03:23:23.875860 140358405149760 decoder_stack.py:344] dstack: Final layernorm.
I0122 03:23:23.880481 140358405149760 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0122 03:23:23.943688 140358405149760 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:23.943831 140358405149760 decoder_stack.py:333] dstack: autoregressive generator.
I0122 03:23:23.943904 140358405149760 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0122 03:23:23.944024 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:23.944099 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:23.944153 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:23.944233 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:23.946988 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:23.955889 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:23.956339 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:23.958614 140358405149760 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:23:23.978563 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:23.978683 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:23.978754 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:23.978823 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:23.978936 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:23.980309 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:23.980424 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:23.981549 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:23.984873 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:23.990455 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:23.991386 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:23.991472 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:23.991510 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:23.991563 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:23.991739 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:23.991835 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:23.991876 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:23.995868 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:23.996003 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:23.998202 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:23.998287 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:23.998378 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:24.002425 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.005665 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.005801 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.006135 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.006256 140358405149760 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0122 03:23:24.006425 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:24.006500 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:24.006566 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:24.006654 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.010016 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:24.022103 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.022896 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:24.026740 140358405149760 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:23:24.040766 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:24.040864 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:24.040910 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:24.040950 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.041038 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.041401 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.041485 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.041848 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.042566 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.044817 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.045203 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.045296 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:24.045347 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:24.045420 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.045665 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:24.045806 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:24.045862 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.050636 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.050734 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.052840 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.052924 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:24.053022 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:24.056457 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.059566 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.059673 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.059922 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.060013 140358405149760 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0122 03:23:24.060109 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:24.060158 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:24.060194 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:24.060255 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.063068 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:24.072484 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.072928 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:24.075158 140358405149760 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:23:24.086775 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:24.086856 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:24.086900 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:24.086953 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.087038 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.087443 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.087536 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.087981 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.088909 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.091106 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.091422 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.091502 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:24.091542 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:24.091608 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.091827 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:24.091954 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:24.092010 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.096033 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.096123 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.098225 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.098308 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:24.098419 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:24.101817 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.104837 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.104927 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.105171 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.105260 140358405149760 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0122 03:23:24.105362 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:24.105410 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:24.105446 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:24.105515 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.108264 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:24.117201 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.117606 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:24.119801 140358405149760 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:23:24.131494 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:24.131570 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:24.131615 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:24.131666 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.131748 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.132152 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.132245 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.132693 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.133614 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.135852 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.136202 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.136296 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:24.136352 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:24.136441 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.136740 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:24.136888 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:24.136950 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.140575 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.140671 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.142935 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.143063 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:24.143192 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:24.146838 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.150563 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.150710 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.151014 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.151114 140358405149760 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0122 03:23:24.151237 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:24.151287 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:24.151342 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:24.151422 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.154245 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:24.163601 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.164072 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:24.166296 140358405149760 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:23:24.177875 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:24.177962 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:24.178027 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:24.178092 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.178192 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.178609 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.178703 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.179171 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.180115 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.183046 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.183369 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.183452 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:24.183492 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:24.183558 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.183786 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:24.183912 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:24.183960 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.187886 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.187979 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.190060 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.190144 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:24.190242 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:24.193865 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.196925 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.197021 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.197267 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.197357 140358405149760 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0122 03:23:24.197455 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:24.197503 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:24.197538 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:24.197597 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.200351 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:24.209860 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.210266 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:24.212559 140358405149760 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:23:24.224000 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:24.224093 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:24.224156 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:24.224205 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.224316 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.224760 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.224869 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.225350 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.226288 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.229125 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.229454 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.229568 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:24.229608 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:24.229670 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.229940 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:24.230096 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:24.230165 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.234114 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.234248 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.236396 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.236498 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:24.236595 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:24.240009 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.243054 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.243146 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.243391 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.243480 140358405149760 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0122 03:23:24.243578 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:24.243626 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:24.243662 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:24.243721 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.246482 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:24.255540 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.255990 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:24.258190 140358405149760 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:23:24.269706 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:24.269783 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:24.269827 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:24.269890 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.269972 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.270380 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.270474 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.270928 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.271605 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.273786 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.274097 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.274178 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:24.274217 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:24.274277 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.274528 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:24.274658 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:24.274714 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.279407 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.279509 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.282453 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.282555 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:24.282659 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:24.287693 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.291605 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.291710 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.291963 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.292068 140358405149760 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0122 03:23:24.292171 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:24.292246 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:24.292282 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:24.292348 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.295194 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:24.304079 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.304492 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:24.306697 140358405149760 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:23:24.320635 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:24.320751 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:24.320813 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:24.320868 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.320974 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.321381 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.321476 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.321901 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.322813 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.325224 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.325644 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.325746 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:24.325804 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:24.325889 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.326118 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:24.326244 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:24.326295 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.330237 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.330414 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.332968 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.333074 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:24.333167 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:24.336743 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.340266 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.340410 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.340777 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.340879 140358405149760 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0122 03:23:24.340985 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:24.341033 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:24.341085 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:24.341157 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.343481 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:24.352501 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.352924 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:24.355120 140358405149760 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:23:24.366878 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:24.366957 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:24.366999 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:24.367039 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.367123 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.367425 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.367500 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.367812 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.368464 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.370630 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.370936 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.371010 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:24.371047 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:24.371098 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.371272 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:24.371388 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:24.371445 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.374559 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.374644 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.376714 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.376790 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:24.376879 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:24.380558 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.383564 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.383651 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.383889 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.383972 140358405149760 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0122 03:23:24.384063 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:24.384104 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:24.384139 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:24.384185 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.386109 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:24.395062 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.395528 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:24.397725 140358405149760 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:23:24.409691 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:24.409777 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:24.409820 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:24.409860 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.409939 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.410252 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.410327 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.410647 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.411314 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.413706 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.414020 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.414098 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:24.414135 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:24.414188 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.414372 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:24.414470 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:24.414513 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.417658 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.417742 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.419826 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.419903 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:24.419991 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:24.423357 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.426334 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.426417 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.426661 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.426744 140358405149760 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0122 03:23:24.426834 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:24.426875 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:24.426908 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:24.426955 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.428937 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:24.437992 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.438422 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:24.440619 140358405149760 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:23:24.452166 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:24.452247 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:24.452289 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:24.452328 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.452398 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.452697 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.452779 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.453092 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.453741 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.455914 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.456241 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.456330 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:24.456367 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:24.456416 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.456587 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:24.456710 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:24.456751 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.459840 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.459922 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.462044 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.462122 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:24.462212 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:24.465686 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.468755 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.468859 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.469109 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.469206 140358405149760 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0122 03:23:24.469297 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:24.469338 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:24.469372 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:24.469419 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.471377 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:24.480312 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.480708 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:24.482918 140358405149760 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:23:24.494487 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:24.494570 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:24.494612 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:24.494652 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.494721 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.495064 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.495151 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.495462 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.496103 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.498229 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.498567 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.498657 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:24.498694 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:24.498743 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.498913 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:24.499006 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:24.499048 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.502140 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.502223 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.504287 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.504363 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:24.504453 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:24.507973 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.511026 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.511114 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.511352 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.511438 140358405149760 decoder_stack.py:344] dstack: Final layernorm.
I0122 03:23:24.513953 140358405149760 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0122 03:23:24.583167 140358405149760 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.583302 140358405149760 decoder_stack.py:333] dstack: autoregressive generator.
I0122 03:23:24.583375 140358405149760 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0122 03:23:24.583482 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:24.583528 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:24.583568 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:24.583641 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.586096 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:24.595039 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.595570 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:24.597744 140358405149760 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:23:24.608953 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:24.609029 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:24.609088 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:24.609150 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.609226 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.609552 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.609633 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.609968 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.610642 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.613008 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.613339 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.613415 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:24.613464 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:24.613535 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.613734 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:24.613861 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:24.613918 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.617022 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.617105 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.619210 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.619287 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:24.619392 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:24.622749 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.625752 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.625836 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.626094 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.626176 140358405149760 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0122 03:23:24.626282 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:24.626325 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:24.626365 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:24.626444 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.628430 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:24.637570 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.637970 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:24.640426 140358405149760 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:23:24.652181 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:24.652270 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:24.652323 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:24.652386 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.652483 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.652817 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.652899 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.653353 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.654033 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.656233 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.656573 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.656652 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:24.656703 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:24.656783 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.656990 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:24.657107 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:24.657157 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.660660 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.660794 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.663295 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.663418 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:24.663537 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:24.668249 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.673177 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.673346 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.673707 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.673822 140358405149760 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0122 03:23:24.673948 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:24.674002 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:24.674050 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:24.674117 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.677224 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:24.691814 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.692510 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:24.695916 140358405149760 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:23:24.712661 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:24.712756 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:24.712813 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:24.712873 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.712962 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.713311 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.713387 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.713741 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.714421 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.717283 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.717709 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.717812 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:24.717879 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:24.717973 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.718256 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:24.718406 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:24.718468 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.723031 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.723160 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.726232 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.726353 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:24.726503 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:24.731328 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.734344 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.734456 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.734738 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.734849 140358405149760 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0122 03:23:24.734959 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:24.735008 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:24.735060 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:24.735133 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.737889 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:24.748414 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.748853 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:24.751100 140358405149760 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:23:24.762311 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:24.762399 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:24.762456 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:24.762517 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.762630 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.763078 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.763181 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.763665 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.764389 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.766551 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.766899 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.766993 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:24.767031 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:24.767092 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.767343 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:24.767493 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:24.767563 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.772207 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.772305 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.774386 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.774469 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:24.774586 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:24.777936 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.781366 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.781471 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.781739 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.781843 140358405149760 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0122 03:23:24.781967 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:24.782047 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:24.782099 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:24.782169 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.784968 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:24.794265 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.794686 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:24.796943 140358405149760 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:23:24.808104 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:24.808168 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:24.808210 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:24.808261 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.808344 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.808753 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.808854 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.809304 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.810225 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.812490 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.812845 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.812922 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:24.812958 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:24.813009 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.813202 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:24.813303 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:24.813345 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.816455 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.816536 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.818628 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.818705 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:24.818800 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:24.822169 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.825165 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.825248 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.825493 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.825575 140358405149760 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0122 03:23:24.825670 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:24.825710 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:24.825741 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:24.825787 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.827738 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:24.836962 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.837373 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:24.839561 140358405149760 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:23:24.851210 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:24.851288 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:24.851329 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:24.851375 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.851445 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.851758 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.851839 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.852161 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.852816 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.855032 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.855366 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.855453 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:24.855489 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:24.855549 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.855758 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:24.855852 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:24.855904 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.859838 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.859939 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.862395 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.862504 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:24.862627 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:24.866333 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.869289 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.869388 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.869636 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.869732 140358405149760 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0122 03:23:24.869821 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:24.869860 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:24.869893 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:24.869938 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.871865 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:24.880747 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.881152 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:24.883350 140358405149760 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:23:24.894745 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:24.894841 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:24.894883 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:24.894921 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.894992 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.895288 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.895359 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.895695 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.896408 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.898810 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.899158 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.899247 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:24.899283 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:24.899334 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.899507 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:24.899629 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:24.899669 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.902770 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.902851 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.904907 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.904982 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:24.905070 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:24.908420 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.911401 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.911482 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.911717 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.911817 140358405149760 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0122 03:23:24.911923 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:24.911968 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:24.912024 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:24.912104 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.914512 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:24.923472 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.923898 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:24.926097 140358405149760 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:23:24.937511 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:24.937572 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:24.937624 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:24.937678 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.937757 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.938164 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.938252 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.938654 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.939325 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.941523 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.941838 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.941912 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:24.941959 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:24.942027 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.942224 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:24.942332 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:24.942381 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.947041 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.947140 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.949229 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.949304 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:24.949406 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:24.952779 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.955956 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.956039 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.956292 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.956373 140358405149760 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0122 03:23:24.956490 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:24.956533 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:24.956575 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:24.956646 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.958594 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:24.967679 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.968130 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:24.970335 140358405149760 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:23:24.981802 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:24.981885 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:24.981948 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:24.981999 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.982088 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.982413 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.982491 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.982834 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.983509 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.985722 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.986070 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.986159 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:24.986210 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:24.986267 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.986497 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:24.986617 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:24.986666 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:24.991337 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.991436 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:24.993540 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:24.993621 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:24.993736 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:24.997147 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:25.000165 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.000249 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:25.000505 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.000589 140358405149760 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0122 03:23:25.000697 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:25.000739 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:25.000778 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:25.000848 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.002805 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:25.011840 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.012342 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:25.014553 140358405149760 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:23:25.025908 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:25.025988 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:25.026049 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:25.026103 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.026187 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.026518 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.026603 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.027048 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.027971 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.030183 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.030529 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.030632 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:25.030688 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:25.030753 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.030963 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:25.031074 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:25.031123 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:25.034234 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.034324 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:25.036508 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.036607 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:25.036730 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:25.040173 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:25.043188 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.043274 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:25.043532 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.043614 140358405149760 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0122 03:23:25.043733 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:25.043776 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:25.043808 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:25.043874 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.045847 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:25.054728 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.055142 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:25.057339 140358405149760 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:23:25.068725 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:25.068788 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:25.068837 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:25.068896 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.069001 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.069329 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.069407 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.069742 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.070401 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.072800 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.073121 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.073203 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:25.073256 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:25.073329 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.073524 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:25.073632 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:25.073682 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:25.176430 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.176570 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:25.178818 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.178905 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:25.179040 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:25.182633 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:25.185744 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.185835 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:25.186076 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.186160 140358405149760 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0122 03:23:25.186247 140358405149760 transformer_layer.py:154] tlayer: recurrent = False
I0122 03:23:25.186286 140358405149760 transformer_layer.py:155] tlayer: compute_importance = False
I0122 03:23:25.186319 140358405149760 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0122 03:23:25.186366 140358405149760 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.188344 140358405149760 transformer_base.py:161] kvq: pre_attn dropout.
I0122 03:23:25.197607 140358405149760 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.198023 140358405149760 transformer_base.py:194] kvq: normalize keys, queries.
I0122 03:23:25.200250 140358405149760 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0122 03:23:25.211834 140358405149760 transformer_layer.py:299] tlayer: num_windows = 1.
I0122 03:23:25.211913 140358405149760 attention.py:418] Single window, no scan.
I0122 03:23:25.211955 140358405149760 transformer_layer.py:389] tlayer: self-attention.
I0122 03:23:25.211994 140358405149760 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.212063 140358405149760 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.212388 140358405149760 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.212462 140358405149760 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.212820 140358405149760 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.213462 140358405149760 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.215620 140358405149760 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.215959 140358405149760 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.216047 140358405149760 transformer_layer.py:468] tlayer: End windows.
I0122 03:23:25.216083 140358405149760 transformer_layer.py:472] tlayer: final FFN.
I0122 03:23:25.216134 140358405149760 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.216339 140358405149760 transformer_base.py:410] tbase: post-attention MLP.
I0122 03:23:25.216433 140358405149760 nn_components.py:325] mlp: activation = None
I0122 03:23:25.216473 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:25.219604 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.219686 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:25.221765 140358405149760 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.221840 140358405149760 transformer_base.py:443] tbase: final FFN
I0122 03:23:25.221927 140358405149760 nn_components.py:320] mlp: hidden 4096, relu
I0122 03:23:25.225360 140358405149760 nn_components.py:329] mlp: final activation = None
I0122 03:23:25.228459 140358405149760 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.228558 140358405149760 nn_components.py:261] mlp: residual
I0122 03:23:25.228808 140358405149760 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0122 03:23:25.228910 140358405149760 decoder_stack.py:344] dstack: Final layernorm.
I0122 03:23:25.231438 140358405149760 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>
